{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet_assignment_13(shiv).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z-cLxgqLNRGf",
        "outputId": "2e3d8de5-f45a-4ad1-f25b-f4e356dc76d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "% matplotlib inline\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import random\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import gc\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.layers import (\n",
        "    Input,\n",
        "    Activation,\n",
        "    Dense,\n",
        "    Flatten,\n",
        "    add,\n",
        "    multiply\n",
        ")\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv2D,\n",
        "    MaxPooling2D,\n",
        "    AveragePooling2D\n",
        ")\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tgztUp8FNQuq",
        "colab": {}
      },
      "source": [
        "norms = np.array([0.4914, 0.4822, 0.4465])\n",
        "stds = np.array([0.2023, 0.1994, 0.2010])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNtSpwQVOzqO",
        "colab_type": "code",
        "outputId": "bc2bc4f2-d5c9-4c2c-ea88-ab11d1e2fc6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "(train_features, train_labels), (test_features, test_labels) = cifar10.load_data()\n",
        "num_train, img_channels, img_rows, img_cols =  train_features.shape\n",
        "num_test, _, _, _ =  test_features.shape\n",
        "num_classes = len(np.unique(train_labels))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGbErbxAsm2V",
        "colab_type": "code",
        "outputId": "6a8abdd9-f4fb-4f49-d996-18fc9c3344cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "class_names = ['airplane','automobile','bird','cat','deer',\n",
        "               'dog','frog','horse','ship','truck']\n",
        "fig = plt.figure(figsize=(8,3))\n",
        "for i in range(num_classes):\n",
        "    ax = fig.add_subplot(2, 5, 1 + i, xticks=[], yticks=[])\n",
        "    idx = np.where(train_labels[:]==i)[0]\n",
        "    features_idx = train_features[idx,::]\n",
        "    img_num = np.random.randint(features_idx.shape[0])\n",
        "    im = features_idx[img_num]\n",
        "    ax.set_title(class_names[i])\n",
        "    plt.imshow(im)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc0AAADECAYAAAD9PXphAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOy9eZhmx1kf+nvPOd++9Do9+yJLo12y\nLGzLxjJeMGDA5BpfG3IvECA49xJIWBKWQHy5SgKYLIQlvjdcthBMAPPYEIfdNg4Yb9iSZVnrSCNp\nZnr26b2//Sx1/3jfOvV+3T093epPmpFcv+eZp785dU6dOlV16tS7/V4yxsDDw8PDw8PjygiudgM8\nPDw8PDxeLPAfTQ8PDw8Pjy3CfzQ9PDw8PDy2CP/R9PDw8PDw2CL8R9PDw8PDw2OL8B9NDw8PDw+P\nLeKqfjSJ6NuI6CM7uP67iOiTo2yTx+hBRH9NRO++TNkhImoRUXilc19KIKITRPSWDY6/noiObbOu\n3yKinx5d6zw8RoeX2vy8qh9NY8x/M8Z87dVsw5cLrtWPkTHmlDGmboxJr3ZbrgUYY/7WGHPT1W6H\nxzAut8nx+PLDNaueJaLoarfBw+Nagn8nPDwYV/NdeEE+mkT0L4joaSJaJaLHiOib5fiQepWIDBF9\nPxE9BeApdewHiOgZIpojon9PRBu2m4h+iYhmiWiFiB4gotersvuI6A+I6LelHY8S0StV+T4i+hAR\nXSKiZ4noB563DtkBNunL+4jod9R5R6TvIiL6GQCvB/A+UYW+T875SiL6PBEty9+vVNf/NRH9NBF9\nWq75YyKaIqL/Jv37eSI6os6/bF2C64noc3Lth4locm07L/O8/5CIHieiRSL6SyI6PKKuvBbwKhnD\nRSL6L0RUJqI3EtFpe4JIOD9ORF8C0JbxfAURfUHmwAcAlK/eI7z4QEQHiegP5V2fJ6L3EdH1RPRx\n+f+czPNxOf/9AA4B+GN5F37s6j7BtY3N5icRvY2IvkhES7K23KnKLrsGy/r2QSL6HSJaAfBdL+hD\naRhjnvd/AN4FYB/4I/2tANoA9sqDf1KdZwB8FMAkgIo69j/l2CEATwJ4t5Stvf7bAUwBiAD8cwDn\nAZSl7D4APQDfACAE8F4An5WyAMADAH4KQBHAywA8A+DrXoj+GVFf3gfgd9R5R6TvIvn/X9t+k/9P\nAlgE8B3SX/+b/H9KnX8cwPUAxgA8Jn3/Fjn/twH8l23UdQbA7QBqAD5k27pZOwH8L9KGW6Te9wD4\n9NUegxGN4wkAjwA4KP33KQA/DeCNAE6vOe+Lcl5F5udJAD8MoADgnQBiAD99tZ/pxfBP3v2HAPyC\nzMUygHsB3ADgawCUAOwC8AkAv7hmHN5ytdt/rf/bbH4CeAWAiwDukXH4TunXEq6wBoPXtxjA2+Xc\nylV7xqvUsV+UBfG7sP6j+eY15xoAb1X//z4AfyW/h67f4D6LAF6uOv1jquxWAF35fQ+AU2uu/QnI\nR+Fa/qf68j5s76P5HQA+t6auzwD4LnX+v1RlPw/gz9X/vwnAF7dR18+t6fuBvDiXbSeAPwfwPeq6\nAEAHwOGr3e8jGLcTAL5X/f8bADyNjT+a/1D9/6sAnAVA6tin4T+aW+331wK4ZOfbJue9HcCDa8bB\nfzSv3L+XnZ8A/jOAf7Pm/GMA3nClNVjWt09c7eczxuAF0QsT0T8A8M/ACyQA1AFMA9jI+WP2CsdO\ngiWtje7zIwC+R8oNgKbcx+K8+t0BUBa14GEA+4hoSZWHAP524ye6etikL7eLfeC+1DgJYL/6/wX1\nu7vB/+vbqGvtGBZw5XYfBvBLRPTz6hhJvWvv92LElub1mvP2AThjZCVR13psDQcBnDTGJPogEe0G\n8EtgM0YDvEFbfOGb96LHZvPzMIDvJKJ/qsqKck2KK6/BG30bXnA87zZNsUH9GoB/AlbXjYPVUnSZ\nSzZKu3JQ/T4E3smsvc/rAfwYgG8BMCH3Wd7kPhqzAJ41xoyrfw1jzDds4doXDFfoyzaAqjp9z5rL\n1/brWfAk1jgEVqNuF1upa+0YxgDmrlDvLID/c824VIwxn34ObbwWccV5LdBjdw7AfiLS8/rQqBv2\nEsYsgEMb2NB/FtzPdxhjmmBTj+5jnw5qa9hsfs4C+Jk173PVGPN72NoafE2MwQvhCFQDP+wlACCi\n7wbbtraDHyWiCSI6COAHAXxgg3MaABK5T0REPwWWNLeCzwFYFYeLChGFRHQ7Eb1qm+18vrFZX34R\nwFcRxz2OgVUbGhfAdgKLPwNwIxH97+Jc8q1gtemfPId2baWubyeiW4moCuBfA/iguXKYya8A+Aki\nug0AiGiMiN71HNp3reL7ieiAOEX9S2w8r9fiM+B5/gNEVCCidwB49fPZyJcYPgde2H+OiGrifPU6\n8PrRArBMRPsB/Oia69a+Px4bY7P5+WsAvpeI7iFGjYi+kYgaePGswc//R9MY8xjYHvYZ8MS7A+z0\nsB18GGwk/iKAPwXwGxuc85cA/gLsrHIS7PSzJXFeFu+3AbgLwLNgCejXwQ4w1ww260tjzEfBi+6X\nwH219uP3SwDeKZ6av2yMmQc/8z8HMA+W0t9mjLmS9LdRu7ZS1/sB/BbEOQvAFb2TjTF/BODfAvh9\n8Zh7BMDXb7d91zB+F8BHwA4PT4PtPpvCGDMA8A6wPX8B7Az2h89fE19akHf9m8COP6cAnAb34b8C\ncDdYO/WnWN+n7wXwHvH6/JEXrsUvLmw2P40x9wP4RwDeB1Z9H5fzXjRrMCDG2msZRGQAHDXGHL/a\nbfHw8PDw+PLGNUtu4OHh4eHhca3BfzQ9PDw8PDy2iGtePevh4eHh4XGtwEuaHh4eHh4eW8S2yA0m\np6bNwcOHEWwQvWTDcjIluWb5OVw2HDBp5DqsK7O/bZ1aGs7kpz1mNgzDXN/A/D60wflr6gSANLP1\nr5fESep3dalzDHDm9EksLMxvJT50U1BAJgiDoXZZ1t1QBiGM3L7HBnAYaXtqMnddGAy3WZWFUhaE\n4dD1/Dj8O8tsdIi6X16Hfn6uPwikTtXfJMdsXaQmUhjYejd4LrlPHGfrbkdk0O8lSAbZjvu72ayb\nXTNTw9Fgdt7kc0Tfm9sYRdHwybqN+cRz/W3HM7P9PDRd7Y02aOAGWqG1R/T/3ThuUFk+D/hPpofQ\nXP46e2T21OycMWbXBq3cFogCQxQMv5dr1oShVti2bfQer2mlrnP92WqOm+FjZqgvsnXHKpUKACAq\n8PuSpuo9kxsl6UBqVO8urV03aF2ZrYAw/N70O33Eg3jHc3y8UTF7p8eQpevnY96GIFhX5vphqHOG\n6h5618m+6+vXCLt2kbxU+puRyGscKHrxMBhun1Fltnr7Lm2sOV0/Z9bOraHHAjC3uIrVdm/D/t7W\nR/PQkSP4+Gc/jwhqkkhj7YLYj11ZnEnHyUIaqiZE8juSDglUnSVZMANZoFP9MZOfvQEvvO3eIC/L\nYBds91iZDHbE8xuhaoSd7CbhY0ns7tOX6/IPT6Y+MnbQ7ECpshQZvuXvvRmjAIWE0ng49FI2GvzC\nTuxqAABKdcfV3VvqAwAi6bdut5WXFYsy4QZMhFKIinnZ5K4pAEC9wQQ/xVS/6Px3tcd1G6q4+6EL\nAGgNHIlHZ4XPIxnzaqWQl9m2dnrcrnqznpeFIfMyzM0t8/NNltx1Ja7r/Dl5HjUfpiareORTz4WP\nYT12zUzhZ//Dv0SRwvxYtcjtsPO80+7nZZG0eaw5AQAoFFybjbz8YchzMXJF+QI0GPDcLRXdWFjk\nOQmMm6+RTGKdr6DX7wEA0kTmSODaHscxAKDfH8h1ri67qRnIcw3idF1ZkqRSj3vHbB3/6zu/eSQs\nRBQEKJab+T35/jT0d3ghHC5bs01QZwBRqD88coZd/NUuwW7i0pTfjTRxfWH7UDfhtttvAQA0J/gd\nbLU7eVlU5PrPL3L3xEnsyiJ+F4pF/huE7t0gGTcD/htG7r0uV6t47FOPYBTYMzWGX/vJbwdSR4g0\nkOftJrIWVxt5mZH5nw74XS+qzV8pkDUy5jnYG7hnzbfAA35fSnB9NF7m6ypVbkMSuX44t1zj61L3\nTkxUeP6VmyIM1Fz4/VKb36/VFa6z11GbAbm3ybhdFLl3wwpbJO9nWHT93Wp38FP/6Y9wOXj1rIeH\nh4eHxxbhP5oeHh4eHh5bxLYJ201mcP7ipfz/Y3VR6ZVZnE5TpeYBi93GqpiUiiMo8Pe6VBJbm3Hf\n71ypsoFpJf9pbahKNdrtszheqzn1QijnWdVLkqy3MVj1sVGqrUjqDXNbg+sqa99CsN6uWgijId3+\nThAQUCoH6HRcn6YZP4dVWdWUSjA1/PzFAvf7gRnH/13PuP0EVq9WG84cNTY1w3WNyVhWXJ2J7Ks6\nos2hglOpBmVRn1M3P3bpAqtKH3/sfgBAP57Py8oRt7lak7kSO1Vnt8c36AxY1VNsu36IRW1eLnPb\nE6XyMmYjq/NzgzFAnJohtV4g6rKi2HvTyJV1e9yO2VlOfxmFTqVkVbWhqOAiZb8labFVzxYKTj1l\nH2a1wx0QFVydVtXr7MtOlWrnYH/g1G59UU/Zc/o919/WjmTnvrVnA0Ai77BWmVqUdFtHAQMgy5AN\n2RiH30v9Oq1rk7LNWy8KOyMGcYa1cCpqVWbtdhvYxfLfSmUfFngcDNm+c2tDGCTSzmEbpUa+3mi7\noh22/Hw3xnqN2zkMiBJQ0dVZkOdBwOaG5b7r4wcfeJxbIyaV/ZNubb35yF4AwN4pzruw2F3Jy1or\nbGaJAn5HqqF7nshwXSRmA5O69aZZ5TZkAzdXKeF6u8t8rKGyx07XWVXb74l6tqf6SsYihPxVa2VU\n4EpCMY00xibysvjsuU3XcC9penh4eHh4bBHbkjRNlqHf6WN5eTU/Zp0YCqWiPUldwbuLUKQ07XVr\nv9bWgSFUO0hbQyC7vEQbra3DgkiF2rHH7u6KRfdYztlH6ie9i5RD8v9I9YZts/X0ilRhvlcVqVXv\n7gthkF+zU4RhgLHxKkhJcqWSOIPILqqkdsUlceApllgauGHcSZoTjQMAgIVoHAAQk9sx9ooswQ2k\nTwnKuSjlflvtcb/rXWGD+LzpZi0/dvMdd/OxfTcAAB56+OOurgFnZgtjdgporyzkZbGMelkkq1rF\nSVglac7qsji0hK6/i8UII+puBGGERm0S0A4NJI5A0jelkpO0MsPntcU5KB1y+pCdrDilaakitFJr\nws/TWnE7dKsR6couvFguqzKZ+2oXHEldVnJsd3t5mZVky1LH0pLOumSknfx8xZIb11Zr2OGqpNpQ\nUBLpaGCQmRRkNqh3A+9i6xSYWqefISlsrc5Bla31SlXrQL5mmctLmnqORSJZOgfp9Y5zJpeSlQbN\nagXk9Nx5SzUhF2gD7d26sTP1c0EQEKq1AuLUrSlk52jAc/uhBx7Nyx58iNlLD0zzupGtuLW/dZY1\nLDcc4oRKu4+4TH/lSNYL0SYNest5WRKLFEriaKacC8Mmz8OAnGTaa3GGtrkFvndzyVFa7zl8HQCg\nIetaV0m0cRjLM4vDktLQnHjmaQBAR7QvN91yW142GAwu44XL8JKmh4eHh4fHFrFNmyaBiLC87HbG\nmeyMpyZZJ0yZtr/xF34D00i+tbJhHplyZbY2m15bdN+hq6BUEZ233cGpXffx47wruv5lLoPPxLjo\nqgNrA1VPIztE275CQYXSiGRQq/IuqNdzUgTBxjyKXVbHFI7S/kDctpndjug/DCWEpsb3jMlJFlFJ\nbF4iDSydc5JFWOAd2dMSarI0UOE1UgeBJZManJQXGJYyYhHugoLboc6JPfpc1UkiDQkrGRtjm+l1\nt35tXnbu5EMAgMWTj/GBjtu1hgHv+EJxR0fsXNRrTR7zVdkdV4puzCfGSwij0YiaAUJUwgbiQIUx\nGWvv4V24MotgEPP8dMoV147YakQyrivLnHRcFqnO2t56yn5jJU1rL9MhC90OP7+2gQaVstTPdXW7\nrt/sMRtXGChxyUpCVRm7SLn9RyFrIXo9O7e05HWlbG7bQxCEqNUaWibMpWb7LhmjpS7+be18tJHD\nw4ZCwvB7uVGdw1oyW2bb6d5xG5+ZuzvouEaRkPJw2w3CGq3GQCnQkIfSyDoY6sDZLBuZqBkEhFKl\nBEqUZk9CYBYWeB4+ffJ8Xhan/Kx9WadTpWloi6PDiVn2cenFzhFhYpK1T1Ei60XXrUXW92I1tqGI\nbh2oyHsfFtUcz4b9ApaVxmS8yT4Ttaq0c8WthyvSvpr4Qpy8cC4vO3byBABgeg/7c3RUf/cHQW53\n3ghe0vTw8PDw8Ngi/EfTw8PDw8Nji9h2yAlhmHKr02Fx2LprF1TYRkGcdJINDPq0xjA/zKLF3/IT\nJ04AAGoN52gyPsnsNfOLbEw+dcaJ3A8/wmq/iYnJ/NiUuEMX5D71qtsnpKL5SgasEnj/f3W5rY9e\ndz0A4HWvuxcAMFDMQ6Go6jKsp9FKTJazEO0URISoUMxVdgBgYFV5PHQD1d+hOGMtiVrt4rJrx/Qe\nPq8nKotVp/XDwDIwFbisR+5+JL7wVl0RqbY0xAms0nSqlPNL7NxzaZnvN1av5mXjE3cCADrnWY0T\nKjXozC6uayVl1YspOjXjgb1s5N89wWpqEyljP9IhteNOUCgUsG/m4JAqrlSW0BHp5/lLF/KyxQV2\nULDhHpkKtxqIM1E3kzCgohunkswfu2WNVBnEOSTIQ0GUI4mclqRu8DJj56L9v3JKkX6xIQXVmhsL\n+zwVUe/qF7BStepjrrXTcWOhwytGgXqjgXvf9MYhZxr7y/ZnX6mvY+ukJc/Z6zpzQVva2e/xsSTW\n1/Fcs+ruJHHzOBXWnyxJ1pel1oSj3jNxCrTvpaaks2FoVgWbqcXSMg1luTZYjS3ZcBkZ/1S3IcMI\nXYEQBGUUFQMOlfl5el12sGm3nYozlrk2kH7L4N51G/5mQwvnF908We3zs+5pihNoRz2PzO25Lj9/\nWc3xhrD3pANldipwWEmhJmxBijFzeYlNJLWUn2f2pPse1KYO8b1LbG44femsu67L7dldYtXt3KIz\nOXYXO0NzYC28pOnh4eHh4bFFbGvb2Ov1cOyJY/kuFXDOPsefYhfe0LgvdL3OO4PpGQ6CjZQR2W6I\nrVSmCQIC4Qi0XKiPPf5YXvbkcb7P088yt2OqDLb1JkskH/rQB/Njd9/1KgDAikhA1x3cn5fddcdN\nAIB/9977AACzJ4/nZV/7r3+G6+/zTka72jteUGn7kJScDTsn7ADGGPQHA/S7TrKwtJBpm4+pSAF0\nRervCFHAak1J3OLbsxvCE6l4Xy3lLokzSNZzY7gqQc2JDFjDuClTkHZNGuc4NFMW4754DlHHSQKF\nAp9fPnIEALBr3HFI3iTS6peWH+b2HXK71j0z/JDTBZaUVozrj7lWD4VoNHu/UqmE6152RDnAAOOT\nPKfsfO32nNNCYsM2ZNfemnekH0lJeICn2B2/NDHuyjrLUqc4dZWdBBjE1ilLJlWoIrll97606hwu\nlsRxYrnFxwaq7TWRImtjEs5TcXVZrmdLZDBQfMNpKvPIapGgtTOjdQTKsgTd7jJCxYVsn92SWBRL\nrqxek2cSqUMH/tvzjUhAwzz4NuxA/iqtQJokQ3+1lDEQwhQdgjA21hy6X1J2dbV6rH2IRapMVfuy\ntUTtWmUnDkT5GU4JgzRONg2B2A6MAeJBAO0YZeR9qsga12y4cLSLHZbAFmPhs15w7/O4hH6VKlym\nw6OemWWNTEc4pKdUGOCyaO2enhPu5ZK7X7UojnADTfzAbRibZKlw95Qi/BDigq6EsWhCi2aF6z0z\nz+9sV83xqWnWQE6P8xoZqf6tloubaq+8pOnh4eHh4bFFbEvSjOMBzp09i6mZPfmxluxwFy9dBABk\nfZdZY2Kcd9cTE2yHLBZcEGti00OtSYnDv/lPRejc7r//c3nRw48yrVO7K4HbVScxHRLX4g/8/gfy\nY5++ka9tSOhI1nW79JtexsH/Z06wJPv9//i787LuKrsyf/7EMwCAIFD0YUKNRoHseBTFHihATwWY\n7wQGLMXakAEAyBLJIiI2oELq+u36EvfzguG+uajsAmmfd2u762wXbE+6ndVgjndi9b71r1fhERP8\nrE15xsOBa8sRyY4y3nA7v6b0TTGz2WuUBsFKTULFNakzTSzyznRimc8/Wz+Vlz05NwsAWIp4Fz+h\ngqGLWQwakQ3ZGIPYxEPhTzYsxNoW9+5zc785xn2ZiOv8sYceyMsefupJAMCN17M2Y88NLgxq+QmW\nptuz/IxZrCQ5shl3eAwWV9zOfvY8a0vaikyjOsbv2PkleQ/nLuZlNhtRX+bBob2OOjG0NG0S2K4l\nzVxiC4UsQ2VhGbLfjQBR1sd05xgilVnDEmqcu8jvYGm30w5lKUvll+T9TBT9ptU65aEqan5ZmyFh\n+C/gsi+Fkc0+4uZsY5wl2oKy5VpJtiChJ1VFO9mL2SafJcOhMQBgcl8BobTUwqOl5LQhe0PjsXmw\n/XaQphmWlzsI4LQ1MXjNTuX9P3xgd172tEiWp5f4b6TW6WkhIDGifJlSWUSWJNtImXheFpqu7NwK\nP+MJiRwJioqgpSCEGoGTWvtdrqvQ5zbfqObgzdOyLottu6ooVBfbvA5+4ZFj/HxqPrz8Vn4vp8R+\nn/bVml2iTQlTvKTp4eHh4eGxRfiPpoeHh4eHxxaxLfVsQAFK5RKKBXeZ9RYuCbPIUnsxLztzmrkJ\n77yTRd+qUjPaMA3nwu5E57K44D9zktVyTzzpHHQmJOTk5MPMjxgsO3Xwrj2sxjl8/Y35Mevk0hFG\nn6ULLmSgGrKq80d+8PsAALWyU3slHVb/FMTQvLDoeFJ7A5shYn2iYBNnGPSdSm0nIDAjUHvVOcVY\nJYf1fTl6wPHLvgmsOjzV5zH4s9WnXZtX+Ngr6tx/RxecO36pw88zJuEVzaZzWtk9zYbypqjxShfd\n+DaEfSNcVu7hopYiUS8ZzUIzOSnHRJ21rJ5L6q+NcYLfJxfcWDx56gQA4KEy63Mmak4lP12pIBmR\nxjBJUywurw45kDTHc9oXbqdyWBkbE6cUcVjrdZzq/6/u5/l56aFnAQC3Ng/nZXcf5dCb6UPMBxwE\nTr13UUKpPvGJzwIAzs65/m6JWtbosA+b2FiciXSiaRvicPYsM7x0VhyTyqED7JxXkHADbR4xsKaT\n9VysOsPKSJDGyOYvwqw6J6qq3PdlwvQUKccnEjakwFIzqSwwgSRMNpJtZpA6lWBfsvwkebiPU/9Z\nzlU7j7p9lUVH5Aqt2gtkLqTJepWp7Z5EHKaGNLC2OTmrkWufVefmzk9K1ZnGyTC10A5hTIrMOE8j\nG8YWSL/ffNg5EFr/x0ee5nCNZRU6Mt1kVWhBQsDOnHNMQhX5RlTKXEGcuvtFZb4uqvL4zrddWVU6\nqRyqZPfSF215vxoqXu6mKR7HqQl2zqrUD+Rlnz/B71JTQhAP73Nq55sO8fzPhNeWVB54A0Lo1bMe\nHh4eHh47x7YkzSxL0e60MBg4CWFcdhsNkTSDzPGkropzwhPivHPLHbfnZQWROm3QbHvVSYydNtf/\nwIPsoDOvJJK27AIjcU5Qm0I8+riEvRSdC7+1p69K1oj6pGPiP3ozExjU65LpIXMSYr/D7ck6fANS\nDjcd2RmVS3yf+lBWlT6CUYWcwCBJE2TKaaEowewl2UUnymHg02eeAgBc6nF/JZEra61y4PLtMXPQ\nTs673V1FgvltyEljyT1r2GYrvyEJbl51QcCxGN8TFWxsMybYv6nasg2WWZqg89xv5VXN9cu7VelS\nHAlcOMorVlmDMCuS0ykVctLPUsTpaPo7DEM0G42c4xVwApjdXQ5xC0tZb8DtWlYS0d5DPLce7/A4\n/c0XzuRl03exQ87B/fzutFpO6n/oCXY8m52TTBA11w9FccYaqJCIWCQgEpEgUoHiA3Hbt4QHz4rm\nBgD27OWd9oE9vDPXYTY2zCIPvdBOepmKhRgBssyg009QUO+xzWca2uwUisDAOn1ZJx8tAebXi3Yj\nVKFIFemXSCSfqOoIU8ISawpS8DrQKbg1jCrMTVqsu3EISioMCMMEBh3R8tjcp11FimLycIiitM9d\n15Q1xDpIdmJ3XZqkIxM0KQBK5RAg9wxFmd3lGvfbhHtUHJjmd++mA8zhffqCC7nK5B0/d47n9vKC\nK5uaZo1WRfiweyqExGbNGRefnXMtN59L8kmKlEYH0m/dRNaUyDXQZn1qisakNOa0ZDfs43G9ZQ+/\nbyUVZkfilGXk+ky1DyYa0tishZc0PTw8PDw8tojtcWIFAaJSBU8eeyI/1F5lO8niHEsRr7z7rrzs\nnle9FgBgN03HnnwmL4tlx94T6qtu2+0mv/ggZ8N4/Cl2FTbKdrAwx/criE0jVPZVu+HTtF+h6MPH\nG7w7Kak8bZfmua5nnmXb61RD7TZklx1LMP/EjLMdpqWetIV3lYUh/+QIw2HVzx0EQhgFqDedzasc\nc3t2BbxroiUn3VwUe3Jb3OtrPfesDQkNKklIQnHeXZcJNVkkO+iByhlZkB1wJNKo4lJAWe6TKMnP\nSF7EQHbygWJfKFp3cJFoB1XXvo5ISi2RqtsqPGm8eRQAcFBsqBcvnszLlpuLQ4HqO0E86OPc7Imh\nXWZRwi5K8hyxksgKYk9bFs0IZa4f7n0Nk2okT/O8/rtTzp54/jw/4znJY/rXf/t3edmjx9l2VJEQ\njEjZ7AYS5N1ToURBgcfF2tH7ynU+Eeq4VNq+0nFlF+dZkn3tVzLVmFHSupU6bT7OOHbPlYyory1a\nscFnTw9QKyipULLYWHNrWbEMFkVitKdX1LbfSph2uSiqploiiq5oiYI5J90D/N6klg5R5fbsi0an\nGzjJNBtjG5mVaoKykookdOSum24FAJDStHRFe3X+zGl5Pvee7a7xXCChgzu14tawQZaOjjAlM4jj\nHkKlkbB2+rJ0KpWUXVz8Nq6XULX9oqEAgK7MjzDh/otSJwHWKzx41gTaGrh1elGylPQj7tOpMde3\nFdEONFU+XSPhMX2R0Be6Tgq/0OPxmanyWPQVGciY2DvrokFc7SgtmdBbStQQQhVWxeZXL2l6eHh4\neHjsGP6j6eHh4eHhsUVsS8pUTbwAACAASURBVD3banXwic88gGbdidMz+5npZHKaxfaJ3Qfzsoef\nYnVsWRxzFubm8rJEPHiW5ji0Y/ZZpXJbZtVRb5UNy6FSs9x0A4eTdEQ18LRybrCnlctOtVcqCi+i\nzbiieFXnl7gN5+ZYFTLWdGwvMwfZPXlumUX7J2edS/zJs8JUImqNC3POianfbqOjvZN2gCAkVGsl\nFIpKddXn31N9Hrq6Ch2JRA0XSuLXWt+pGI6mYihfZtWIduayRvC6qO+Wy8o9XBgzqmVWvYShU2PE\ngbjVl5R7vKTiiEXFVtvnWGg6kvD13Cq3eUlpnMKD7KDVE+7ivYcO5WXm7CMAgIOfYgadVujUQBcb\nMcLAqT53gn6vh2eOPYqiSoC7Oi8hSqKyLSoO4nKZ34NETA1NpaUbJNyX4zZX+tyJvOz0MT74+QXu\n24eefDYvS0UFVRF1XEeFsbRW+b1IVULrUkHm9aAtz+DMHEbCuqzjkJ6Xz86yinBR3rUZSSIPAEay\nVpRFRZYo9ezGGeWfO9IMmO9kmMfW4oZCsk5C/H/1auRZlaxfXk2pdcui9itKuEitoELcpLJAvG0i\nZcIpyO9K371n3SV+/y9K6FlbZStJC6IelOwZE7ucY8pkmc+/Xu7dJeW0tSLMOWJOKg3cu9unZIjd\naGcwyJJ0yKEtk4wqJCEwpOa/EbVl3Je5pNeNHrd5siqmtqr7nCyIc9uyrFNdOLVpS/orknlWDFxb\nIlFvF9UaXhWV63JfHI3U+MzJGrcYS9tVuF8q7GlG1qKK0vOTXFcWk1RBrWu97sBzz3p4eHh4eIwC\n25M0O1189guPYEK57t4onJp33MZB6UtttytdXmHD64kTzLXZWXASQUkIARJxAOqokJOlBSYSaHX5\n2GpbuTK//OUAgII4K2SpkzQrdd6dlNVOKZOdUUsk05riqq3U+ffcEpfNXnC7lPufeJDvLTus0xdc\nkHlbjh3Yy+7YodpptjrG5Q/dIYIwQHO8hokJF0JT6XLd57/IkkI/dY42dXEsWJGwkEy5bT8R8+54\nb5d3Ww3FCNAd537IxA290nDSIWzgvhAKDFS2jrbs+g8fvT4/ZskkLrR4fO544+vc8+xmaebcw+zg\ndXFhOS87eCMH/7dPnuD7dN2OtvgwayHu/BLPi2SXm7ZnBsXc+WinCAmoFYM86wMAxF1xvun15Rw3\n1vUaO0cURdJOEudos9ISh7VVnpOVJadJOTnPYzE4xPMnK6ksJyJEJuLs01OOPe2WZEdRDkeiCEAm\n56cqHMVmC+n2bF1Og7CwxHU9e+KYPJdz8EhE8ogkb6TmT6VoNH2d10eEqFAYykWah1e4H+4CkQBs\nqM1A+yXFm713NvOJzY+pHGEkqD/KpVdXT8FKpso5rhLaMJ90TTuRez32JCflxVW3dhnJ5Vuf5nmj\n0t2iKZLpQEgX4titN2QGoBHFnBARSoUiUpWTtS9c2b0WHysoTWIojkCWm0A7wsUyNyObtEW9G4vC\nY/vMvGQhIa2hEY1YbO/r1n6bCSsrujZYYpFAtFBTY04KtTmEF1vcmZPk1vCaNCcTqT1Qc5ckfKvV\nlnmhHKPSLMNmUWxe0vTw8PDw8NgitiVpEgUIijWsKCql+x9iAoLPP/BFAMCY2qWUhP2/1eLd+ozK\nZBAJrZ2VPg/sdyEdJ55mW6jdwb/5q96Qly3LrmjvPg7Krj7l7EGZ7Kw7SjI1oneHUGVpirh6kwNw\nO2LvOz7rqPLaltZKriuPOXtnWcjsFluyq1FZ1onKQ/kHd4JiMcSBw+OIIhXScVq2fEvcDysDV9bN\nhvMCnlM2hn6Vt05NkVLGy07iLt56MwCgI5L39IwjgKiLViGQnXA/cNLr7v1MlLDnliP5sfEJLt89\nx+1LdjlbmSVP2HUdn78w++m8rHk/S/Yzj3+Jn/PpY+4+YgsvzfH4TBZd2/dffx2+GLiMKDtBEBCq\npUIuVQIujKEqYQVxT9uruZ9DkciKkZP6my2WQuoLTC1WGjipel7eh2aH7eZlRcbR6fL7sNrhcR4o\ne2Kvt36+2fZYH4FUZUAJZdeeCGVarKTWflfc9xfYZtve7dpuXfwDuXWSKBtfZTiwfxSggLCRSdMK\nV1oqLApVng2H0eFGlgbO2p9LkZv/NQlNW5WcpLFx11nyB7usdTWloJANLCrKu7w/LGeeOj8SKbQi\ntsCqypXak9CKRKSuJFaZNYQcZiD3CVT7ykk8MkkzS1KsLi0jTpzWwVL3pfK3oe32FQltk7U1GPpk\nSBiWrJ99lVe3I10yJ/N3sePeqbFyV/7ySa3MzT1LJ1hqu+dvVkU7JrlUqeDOf+Apfr8uLfPa8qqb\nHAVgUej9jEiYoepTG0ozL1mEBomTUKNSmPspbAQvaXp4eHh4eGwR/qPp4eHh4eGxRWxLPWsMkCRm\nyBnCqqRCEZlXWs5F3rTZwDsxxmrZ1WXHyJAKG8/SRWaomZ50atPv/J7v5DIJR3nj17wlL/u9//7f\nAQAdcWooFJ2o3hdVZV9xh1Ysj2XBqmcVsaLwQy63WXVQVew1JIboWFQxOvduplz+geFMBkmSjCxh\nLBFQLKTox0610V+VzCJyiJR61hQl8WuV1STtwKlZZoUD80iJn2dMqbzoFKs/qyUer/55F14TCLNH\nQxyoxsmpmybO8XgW5067RovT0niZVY/nH3eZVlqSAHxZeFXDp55yZZfOAQAqJzh58/hJV6dVOxtR\nKe0ml63g+ukjKEWfxWhACMIIQeTUOJUKz4PBQDpc8ezaxNSZqAW7irnKjou5xLycnVXnBJfKO5Mk\n4gzXd+/F3AU+P9f4qSTLA1EblxSTTCzqxvaqqJmUs09FVMqpqLzSgcq+Iw5klrCm2XCmE0PiVCTe\nH/2+e64sHS33rDGGn2sTrk/tV2fHIdtAfUZrWFwqSiY4IGaFlWlJHF13Zb0O92HLqqPVDcviAKTV\n5B1RNYo2c4h71raqIGrZMUnUDgC1CXYAai0Lk5gKq1gRlptVMflEahwblWxk0k2cJLh08WKeqQVw\nXV+UeRUanXDb5NcBQKJUnIkE+fWsWUi1MrUcwcaaMJQTjnwrEnlvlLYVPVmnVrpOdV0W/tqKrNcp\nnHmmF3M/h8KuZZQTaEucliJxOi2m2olN1OglYTorKpU8+STUHh4eHh4eI8H2uGfBOw+jrPaD1O5i\nefdFaidSqYn0ucg7q0TlBgwlJGH3DIc3vOZ19+Rl97759QCA3/2t9wMAPiTSJQCcPMMSSWMXO6sE\netctWSYiJWGFsmWoC+9poJwD+rJVtMe0x3qaO00IH6Nxz2zErTyTgOdeTwX8xvE6SfS5gmAQmBih\nGqbVjgQSG0va4HbAY9P8jEV5nnNLrl1nW9zmS8Tn3Gacm/fEeRk7kUgKejdZ4GOxOAc0Ck4jkFbZ\nCazbdDvmZwP+PX+UuTcrNecc0Ccen0XZOc5X1W76NRy6NKjyOSefdTzFxcwGpItjWcnVSX2ARpRP\nEwRQEAw5l9iQD+tUo3UIrTa3dVyIHypKU9EV1/dWj6XIixfP5mXFaXYqS6Tui5cu5mUXzvF5dQn1\nCYeCroXjVmXZ6EgbWhJmlCqthBEu0Uycv0zqylJxfLChB4HiSLVcpH1xKhoobUY64nyaAQHVKMxD\nSACXv9M69unhXZ/VRPOkym95Vxdi9x52DPf1bTM8//fW3RqxKiEml9r8t6ek+1pDHI+Us0pJ+n9K\n0nQcvu66vOym21891PYnH3vItS+WzEmxdejSnM3yQyStZKA0DCNMpxnHCc5emEe17Bw2a1WW1myA\nf6yIFVLRbPVFo9FTEmA/kfko0mSg5kbBkpRMSkYjRbTSFq7m1T6fU6upkCubO1Pxy64U+J61Se7v\naslJmq++kzNnHRJHtsC49tnQrKRvM+K4Ps0kW08YZVKnyssaFHISjY3gJU0PDw8PD48tYnuSpjEw\nSZzrqwEAYmfKZNdUVYz/LWGzT2SHPK2C7d/6tq8GAOzezRLjqnL5nWtzaMGXjj0KADh91tHv1Xax\nPeuS5Njrdt3uuSe2zMmG20UZsUlWJThfbxNi2XmXhM4pVUHjeWYHedZe19lq+/K7K7v83NWdK4VJ\nRiNphgGhWYnQS9wu92QmxAViaykW3I551wGmqbN8gtTVO1khPpARX0pcCERFwhVstpNi7HaMfYm2\n74n0uaRsms1IyBAiJ/k8WGYbzvIdHMYys9u170iNx/rW2+8EAHzhsUfzsqpkVV8Su91H7/+Mu0+L\n+/dOyYXavcGFJy0trows80aWGXR7PfSV/SqU3zan4ylF22jp5Y5kTPm3e9qF6vQMP8fsPEuO8wsu\nq31D7C6dpgS5L7j5vbJk8zHy/6sVt6uOhcAgVlR5idCGtYViL0uUpCk760QkG6N24YWIxzGSnI5a\nO9IVv4TESkJGh2CMdp9dLxLuPRyhH7s1xUbU2DytfSVd2tOsxKkjgIRzBJlIioNs2E4FAH3JcrKo\n7Fs1SafSEHq7fs/db0UkrWLFrSm33ckEK29/19/n/99+a17WkGxKp46zvX7xtLPpf+5zTPJySfJO\n2lyTADAzyeNck9y+PZX1ZbmfICUVnrIDxKnB2aUE5YLTQtXb3HFjMtVKat1oTlgtCs8Xa0MHgETW\ni1ByDWsCiKqEyZVlrjaVxiQq87P1RJpOhvwEeC0pqexVJfEBGKvy+1IvKdup5C81sc2g5OZxJppE\nO5ppoucuP4cltjBKK1GIok1zynhJ08PDw8PDY4vwH00PDw8PD48tYpshJxnSQR+kDb5CPGh5G1eX\nHKtOLE4JRhwejtxye172rnd8MwDg2NOc0Po3f/UX87Kx69lRotxklUil7VQTVBx22ul0tcpJEiaH\niqU/ELYW696t7LtGPEgGwszRV3ynfQmXGYgq1ihX+6qoIQ7vZbabYtF149mnTiDYYsaGKyEMIkxU\np9FXGQKak6JCfRnfe0/VlZlIHJPEJT5UrD97hSd3IBy/K5dcWMl1orrti5pkSamGrKZKEqHgUubU\nf4mEXESKBeq4HNs3MS1tcGqZVVGHn77Ec+TZk+fysk9/+gEAwIXZ4wCAJ3qO1WlS1DgFYYi6Y5dj\nZwqWu0NJo3eCIAhQqlYQq/kdi66wJfMhVaEOoQ1ZkswxAdx1lr40lkzIg9ip93vCS9taFo7lZReO\n0pcE6n1h3qmUVMYeUVn11ZwfWF5ZmbsB3DzNJHRgaornwb4Dd+Rle/bw+OySLBwD5UAUi6Ob5Z4t\nKBNAEIyWezYKgfEmgVI3hpnMw1RCeEiphyMbQpabRFSYj7yGgaiQu11XZlX4fVFDzzo/uNx0MzbG\n163GrnDXbjF5qCk2JelTjh5iVrJG0b2DPXF67EgmlPMXnAPY7GlW7deE4aYIlfg4FrWsOAIFoVtT\niqUCsyaNAIMMmO0AlcjN45pl7ZE5WlVOOJMyFo2aqMPbbo0siOnGOimVIrduTFYtbzHXPVBsWX3w\nerEkFExLbacatZaWyboLDTx6gN/3XeJYOlVXKuIOvztZn80TZWXOsL5UmUyMQKmIISFIRGKaUvNo\nkJpNHa+8pOnh4eHh4bFFbNsRKIuHDdIk313rojs+5TJkpH3elYSyi42Vi/XDj3GOxI//zccAAIFy\n5vivv/rrAICe8A+WJZsEAESy226v8G4wURLJVJ2N1enA7erttQXZuUXKndxy4vYS3q0U1U6pIPyb\nRw7PSJnb6R3azxkh7n4FOwQsLM7nZZ/NYvztA2pHswOUi3XccvhepCUnyS3PsTPBIOQ27x9z3K7L\nXXYoGaTcHqN2k4Hs9AYFCYVQ26WVVHbmDe6rpTG3K0zF2WlZnLrmi07SWBIpt6vCXi70mMv06T//\nEwBAKXC71l4seVIlcHv+vAu1aC/xOJqSzVupdtYZj91jQo5RfcplDCndeN1QhoKdgIhQKBaGdp1t\neX4raezbvz8vi8hKwNJmFSAdijRxyytZu/I3TzyWly0K0UFrRaSSVUdukMmuvyCeQDq3ZzrgfjAF\nlcVHHBgSeccCOImxEPG8eeUr2fHqwEFHClGS3IJWctSOQDYMJRGHtih00m5ZZWQZBYwBstTkHL+A\nI1wQelmoOPxcmjwlHm21opvjY6IFWFrk8ZtWPCb1mhBJiBZoSZEHLF0UpziRZKoNJ60Y6wjYceve\nqeOsHTt9nB3ZZmZchphSgftq9x6WQscmXJ9fJ1lt7riVy8oVR3wwtyjvpTh0kXIu7PT6OHPeOYvt\nBIPM4HQ7QTl0490s8TPWxAGwrhzh7GyyBDBQjmZV6ftQ3nGd87MhWiHL69vSWWyE1EP8r1BVa0oi\nnl4V9fxTVenTBv9tVJwj3K4D7IRXFKJknU+2JPy/NkdtRm5dswoTSyNs+XcB5u7ONhE1vaTp4eHh\n4eGxRfiPpoeHh4eHxxaxTUegFMmgDc2ESBVhfKixLqQ64dSFTcmQ25C7lFXKmY8/yKmg5kTsL8Kp\nfZZOsdqqsZfj8coqvY5lXWkn7MhSypwqdrLG4rtONA2JISwSqyPK6pHH6qL2laTDX3ro83nZN379\nvQCAtwg70Re+8IW8bGqMxf2ycJQWA6fqGJ+sIoxGsxcphFXsHn8FTNWpFfbsZjXRk89wKrY4c6q6\nuiSPTiWus952/d0Wp5D6NKfOudh2DlufEuagvfcwm8m+Iy6p9JN/y/GSTy2y2uiUerRlcQrIWoq9\nQ1Ty6Sw7PRjlmGK7virMPtqBqraLVYljwvQ0Rk63Foh6dgWskn+k7dThLx+/bWTq2TRLsdpqDad6\nWpOIOVKq0Yo8R3OM55gh96yr4tAzvZ/V+6+4+8687JOfZKenXkeczZRKieQ+JWFGKav7dayDkmJs\nskmVs9Q6ZagUTBKvfOggqw/HJ5yaNROHLsuiE6p3MyrwddYppRCqtGFmVPRLtv1AaGiYYFZQLhtp\nmxsPm/ksk3hBo8ZKQipxYVV4RZUm+fSCOK2JCv3AtLvf7Fm+sCvvzeS04+G9MC9x0coR8LywNp0+\ny+aFtzbcjdIeH7NsQe/+jn/g2td6J5fJOwjlYGVZoU6f4PjOp48fz8tOnZpFFLoUiDtBmhkstfso\nKnV4p8ed2hA1aEet731Re9a6wt6j6LdsnGahYK93DlGB1N8T88GqsupZbt+MeAy1H19B1s5Exe33\nO2y6qc3wmlAvurEYk5jPurAKnTzlbpSJg6MzCyoTQCAMQmQdglxZf9D1jkAeHh4eHh6jwPYkzSxD\n3FkdYseoirTQFCccnYS6Kgmfi8L7WlBu1JHsqA/MHAEATF3nnCFs6Idlvz977kxeVq5y/UdvYsaZ\nxx9xO4ujNzMzx603H82PPf4kZ824dJGlk7riORyXpMtjdd4h3f+5j+ZlDWHm6Auv7PSUk6AL4oy0\nKEwuSex2Pnv27srLdwoDwsBEMH23t5kQabInUsfpOefSXitIYuFl3plpY/bevey2bVJhe1GJnPuS\npDis8PVPKaeVR2dPAAAuyQ7zYs89q5HMAuWy25mPC6dlfReXjU06iXF6ih0fJid4pz0+6fo0kHnU\nFuad8yfdmF+4wLvwVoul3dK0cwxbXF4c4ordKbLAwKjQkYI4KZCxGU1UUmjZMZuWDdFw9XQlLMQm\n3r7r9pvzsiUJuTl7VriYUxVSJXXaBMXtluNr7nb4HTGpG9cosJIwt1kJpjjyMnY4qdQL0hbN5MV1\nBIkN4XBST1ek0L5oYEg5jQSb7MCfCygAogoN7d5twmVj+1w1OxTJ+rpJPqiS9eD8Ev8nlpATzVM7\nLsmhg5CvqyunrcYkv+tpxHNVu/Htn+R5vGvarWvN6o0AgNe+ijUz00XnpJP0WYN24Qlebw5OHczL\nSkdZg9ORvu+oMJ9dkzynyyWeJ5lyWOysriCKRhTqYzikJ1FSlwiaoIFI72q8bXYSG/ISBm4wLAtc\nUQaoXFTzRLRvoUyYggo5ybrCV26zoygnnFJBpFWjEq2LY+fqKvdppBjXor28lrRFa5OqdwMypw2J\nNlO1PRKHsEAkTFIvb1KMNku64yVNDw8PDw+PrWJbIhHBIEKGirLZlcWe1JAg9qayvxXFqJAKz+VA\nufKncudEjhV3Oddsa3mhPrvmX6ekQ5sjrSSSyW13vzovWxEShQcVp2nc4zpSkbBWW2q3EfLubm6e\nJZggdFLUEyJt7ZlhG9uEstUWxZXZ2rnKZafLn9kzg4Le7u8ALPNkub0KAMqSYaErQcPPPPNkXlbK\nJEOA2Bpo3EmAkJCRsQmWMGMVxmIMSzAnnuLMIpfm3c55VVzM+xJyUy84CbUp4UVTU45z1dqfJiZY\ny1CrOzuazQe50uJd4ZmzjtzgzBmWLFcl/CJR4Uk29GF8nOu8/Q5nH4xCs+mucDtI0gQLy4uAIjeo\niQ3dcvxq6tXY7obt8BhFAiBxEvbvnmk3f77mTV8JAHjsMeYlNYmTNOcXWUtQEqkw7jkNTGeVpc5a\n1Y2dtcXEUkez6ebi9C57Twkw72fqOgkVE7sPqaVgIM/fziQMou0kolpBzakRgIgQFYJhQpCcQ5b/\n6p19sWRtuPzc2sRqhWUbrlFUIqNdlizXbkdx1rYlM08sN2r1XKW33/UqAMBbv9bl9L3zFXxstyxL\n3Yf+JC8zcycAAOkCrzflpmt9vCC5OTNuWFB065ptaijv+tSYK3vlV9yNv/wbx8W8ExgAmaGhnKGh\nrOFxYsle3PP3JTSnIiEkqQq9s8lQctrWVedfMtaUeVXgk7pLzkY56AvXtzSBdIYbee8LKntVGIgE\nLBqZgTq/JxPAEowkarbYjCmWECNSmrcolDzJdgKpiVQth7lNdiN4SdPDw8PDw2OL8B9NDw8PDw+P\nLWKbISdAmmTo9VSiz4DVR6E4hSBwVdZF4q3WxW1ZqVmtYTlPq6WTPFvLv5QVlPpzXJyKYnH+mKw5\ndeHqMquvzp1+3B27xK7bRXGSWZq/kJddPCcqN1FBTk25BMuWa/RTn/40AGD3bsd32mxwG8rCDxpp\nDxDKMBioMIsdgoiGMh/bvllaYTXeyTnHITtZ5/ZXJT0RKk5V3hCVXigG+bZKn9YRZ5f+WXYq6isj\nfCI8tImEJkQqWasRNV6v4/gorWpsWXhVG3U35tOT7Ah0cB+HEgUH3J7t/FlWzy6IajhQHh42Se5r\nX82Jyl/zapew/MzpE0NhIDtBkqaYW1xAWYXCFCV9U03mYE+lELLcs6Gdrsq5olS054uTkOJs3TXN\n/XD7HXxs127HDHP+Ij9/Zqza1amKH5HEuUXFdTomznazJ1hNP95wTlIVeSd7kvCdAq0GkzbLexeW\n3VyJLcOTVZ/BoT/oY5TIUoPeYpyHzqjb5kxAmu4215pZVZt6N/qSbDiQ5ywpDuUosuw1XMHJBVe2\nIP25V0JNGg3HavbVX/P3AABveOPr82NVUYGbgaRjm1FcyM8wW9CRO3iOZ1OTedmZSzYJtYS/qPcs\nFaqjtMMqzoIKcZlslBGNKKwKIBgKYFTH2RDCQFI3GjXixpbJGpEqJ6FMbBU27WCn5eaqTfVYH+P5\n2FXOemlQkOulTtJqU+6HUs2tTwW7jomqt6Ocfc4vimNjj+u3Caf5ueQZAuv0o+e/OCPJ/FfZyZBk\nnnvWw8PDw8NjJNiWpBkEIcr1JspqF1C1BAGyS5lbco4Li4tCQCBu+lUlFValjkqNd20FZbWvSEaS\nyCY3JtfMXp93OoFwciYq0DuQHePUtHMqGqywBGOloUh5jVhp1R4rFtwOvt3inZJ1YbaSHeAcgSoi\nfQQ6MDwK0e0N8/M+VwwGMWZnT+PpE45r9eIldrs+d46TGrdWVJYXca02IR8rKGnNCJ/kwhz3gym5\nZ70kjl1dcduOSe0KxeieylYsazuDfipS0YEDB/Jjr371KwE4h6B+351fFInQZr9JlNRmtRD793Ig\n/sGDh/Kym29hN/zX3MMSZkPNoyOHr0Op6KSknSAMA9THGkiUNNWR34Nl4ddUu/CI+Hkqol0hFVI1\nFAsBYDBQUoU4exRl937w0ExetucgSyZpbDN8uDolIgiryuHiwH7up6U5ng833nBjXjY9NiV18NiX\nlERubBC5jO8gXa8dkUikPOwAcGECo4JJgbhtEA75XQxLkYn29LJhPrn/hgqdkOQkoZUS2kpikiEd\nyLvaTV1flIQ8ZZdk5jm4z829qapkAFpyScQr4gxnuqy1Klbd2jXfZCn1ic+cBgCUp9y7lBV5bDPR\n+hQU/+2q8L52hOu4qkJiSuViLiGPAhkImdLsBXYOJDYxs5670k9y+0SFYxnRhiTilDVIFEHNMkvh\nBck+stR3fdQRQpY+rBbLza+CJKFe6qh3UNaJooQIriyr8MSU640i1hIkKkOLDVmyjoSp0rSkorXJ\neaZV/yZp5rlnPTw8PDw8RoHthZyEAUr1GmpNZ/sbk92Z/VusuBCDSFzdg5SljW7f7R76Ehax3OEv\n/mDZ5bCzOQszCbaNVGBsc5LvUxO7os3XBgCpBISriAHs232Yf8h5pHdY8jsRG2SqJJ/MBnbbnGxq\n4zEQqS2WHYwOMQmCCGk6GqoxYzL0el08KJSDgJM0Q/DubKrp7C+W1mr+Ap9TVhJJ9TqW1pYu8S5t\nse12ay3R9bcsnZuyn2QiaVsbgLYhBeLjP3/JZSt5+CGm97MSybwKX7FR6jXRMhw84AK/7xEJ9fDh\nIwCA3TNOWzAmdhFrO05VCE5Ao9v3FQpF7N2zFyvLjlCg1WYNw4oQbtRVvr5Q2lOSsJSByoQxkIDt\nquyOa0o6Xlnh3fdgwOd3+k6LYV30x8Q+rXNJTk5yHaWie+bdM3zeXXcxsceeXS4Li9WEWAmip3bh\n1QrbvaOqhG6kSiMgEnQsWp2u0i5ExdFk8LHIDNCPCQVlb7IB585sqSRGebetiUwTH/SSYQlVMQrC\nLj2pJU5Q+Tttdo5E1pvDu50dsmapERWtWyDZkdI5CZM66/wkPvpJDlV74MGHAABH9rkQnclDks+0\nyWNkVK7ZosTESJrHSc94XgAAIABJREFUISKY3nJrKOxs5wiG+jRfb0XbEKl25eEexs4TLfFK3lPp\n04Gqc2C4DktWkirtlUlbUjfPz9ZA+bPIurbQce9ESYzaRw5dJ613PhRV+TZEokFI+u47MpDxHEhS\n4CR166H9blSk3+uKsCdNw3wObgQvaXp4eHh4eGwR/qPp4eHh4eGxRWybezYZdNHvKfaQAatjBzH/\nDZX3fyAOPSSib3XMGXwrokKxak/NE2n1MqbFYQstUWcBwMI8qwKXltjJqFJyNyyLi3lBqWBfeTer\n/cbGJORCGbJtwt9E1FBxXyVfTVgFECessugr1bJNWGrVsJo9Isuco9BOUSwWceTIEdz7unvzYx/9\nq48DcKqHwpRWK4iqO2aVSK/r1BjHT3L2hJlJdjqZu+DUTSuWhUfUzdRfr7qy3KZaaWFDLlaU48yj\nSzxmVmt66JBzEnrTG94AALjjdk7MvGvaqZYrkmGhENmsAyoxbd7f3IZMcU+mJhtSGe8EJsuQdvuA\nUq8XhZkkCKvSTufsUJZMCZYZSqvpbeYV2+ZQMZzY82xYTbHkzA99CX9akjlPUE44du4qVpbFFjOh\nNCdYDRgqB5KezGEbspGpd2wgzCuxsDQFOoG2OB+VxSkjLLk2hOXRhPdYGAMMYgOjPIFy64C0V7Ou\nBjIDM1HnpsphIxZTQk5Ypv2H8sr52cYbTgUbzUgmmpdzUvnbbrguL6tLJqP+gnMEWljkfg1FdX/h\ntFPn33ozm4++6uaXAQBqJeUUKAxaCxmHoyx0XdmyOCr2B2IOmHd1rrT7SPqjcS40ALI0w5AByayZ\nVyoUxrJcBeIQFKjRSFKbpUSc1kJXa2ZV5XJ+pJxwpiUrUCfhOhcHbt29uGrDclwb+mfZ3JR97mEA\nwB373PtyYIbHsytrX2+gwt/k81azISvKqa5U4klSrogzn0qJM+inm4b4eEnTw8PDw8Nji9iWpFku\nFXD0yC40x90uzYjlmiKRBoQvFgD61q23wF/xQsFJYIFsf0nCSQIlWYRihC3I139c5asbiJTR7fKO\npL3s8kJayerlr7gjP3bzzTdK/XxdpAzS1o2bJEhXpYpDJjsjuydL4vVcqDa0RGfZCIIQ1T/4MEYD\nAkA4evSG/Ih1pvr8A+xws7DkjNu9AUuPJpbnKro+PXmO81ueO3GKzxmsd7+2u6tS4HZylgPSZvvQ\nUlFNMtqMjbuA+n37ONC7LM4Etv8B4M7bmTM2CNbv1YxIDLFIraTCjGz/Jsl6Z4g0S0aW4zFLU3SX\nV4bCKkoiWRnZ+aZqHnTtfSXkpKKkyUh+Wz8e3Ub7/FHIfdRXz2Wl6L6E5XSVhNEVooRA5WFs2YxA\nIqH3M6dByESKtLtpHQZjg9VtkLsOdq+KhqhILBHEKrNLVBqxpElAEtBQSIX1NbE9pt9LG05iT0+U\no5Sl1rURYLljEICCPFNNeJKb+1zO2JtezRqQO29ip8Fi4J63I+tMMnCOKTZDRlH4fhu7nbPPDfuY\nl5b6rGHJem49DMv8bpQkuWQ0cBq0KORxPNdiTVqsQtxmdh8YXZYTAan+ds50lhxCzRP726zpeABk\ny0TC13XmTnvJem5Xq+2bFn7dgfLYMgPu747KHGXDrijgftu1y3Fd16oSLtezmrD1WjLbLEpdH9pc\nvqE8Q0dJ/VkWenIDDw8PDw+PUWBbkuZYs4a3veV12LV7b36sLzuJvs3QrdzT25KobU7CSbqKbi0R\nN2ObpzAk3RRxZRapsN1yOxEblFsWw8Wew87F/mUHOEzhuoOufaHYJGweuEKodp+Wys9uhtROJJMd\niN3IRkqK6MsOPrE2Te1OPdLM9gZZloKUBeIr7mJp7dabbwIAHD9xIi97Vn4PhFCgr6QUSzKQSNBw\nqaCkSdH514XyrlZ2kn1VbHhVsTnW6i50olGTfJqKdq5swy/i9cHydkdrd4BhuH73nIlYodtuJU0r\njWpbVZok7vhOYQAzSFFSVIFFIYEw0i6jXNEzyQZhZE6Fqh/a8h7Y/K966zoQO3ksYQSBop4MRfoM\nZG4Nek6TEIpWR9PoJdKGghwbqNARa5q1YUNWGgVcDtWeSPZFNb9jyYVopX1tX+50RkujBwLSQoi+\nihOzz75BAgpYGdBKn31FbpDRMDVnWnD2/rDJ2rFeyPNz74QjlLjx9lcAAEolrr2z5EJIooL0HSn6\nRPldKnCba8qFIVm1uWyFmlIF9S+dOsb1L7APgYq0QMeuh6I5OLDPUfPtueUelCsu1+9OQOCQnkxL\nZPncXC9pWmR5jlNFU2c1QKm1L6uxsHXJHIpUSBGJ/bEOlrTv2avKDrPdN4Gm6+T27N7FZB2HptX6\nJO9qrcL9PVC+J+02ryH9noQ+KhnR5o+1r3NX5QlOsmjYx2YNvKTp4eHh4eGxRfiPpoeHh4eHxxax\nLfVssRDhwN5JJLFTs5ZFvq1UuKqGUh3FwqfYEE7U+dQZt61zQSSM90XlRm8l476oEIJxpxK0Btyp\nCWY02asyRNTL8jjGGfKtVsAqmNLYqYFs1gfrDa3VEpZ1Jha1s+YitA4pVt2oNcusChiNupCIEIZh\nrvLUbSyJQ8adt92cl9160w3S9lTa7vohyFVX/DdUqSNsImP3/Nm6MqvNyVQ4hlWDBcpxxmbAsTyz\nzWYzLwuD4VgA7UBlQ3ps32o1kFV522O6DckI1bOFKMKeXbvRVY45fWEOsY4NsWYjEucM68neUu7u\nRuaGTVISJ05ttCrOJT1hBNKtLwqnckmYTlJXZR62YtRe12aIqAgT14Ji/RmIDtP2ls7kkIrTWChq\nRJ2EOs7Tm4j5QcWRBaO0PgCojE3h5W/7ZqxeOpsfWzh9AgDQX12Wtupwr2ToWHeI25ifoT7O6rug\n6Rx0BqKO662wapSeOZaXPfXolwAAN0mYiFbjtaSfAkU9VJRMH4mwBbVUaNdKlxmwbMJko1T9OMD1\nj90sLDYtx8oFCfMqhLzWZXWnnl0NJod5jXcAA3b6Gnplcmc1G162/v3PnfHUO2tNFfb8WJXF4oRl\n2aV0Fhu71tcj7sej0+6bceRl7KBVHnfPL9pVdNrcz5nmhrbHMssN7dpeFxauRlWcTdVDl8Q5LpV5\n0Vffps5yb9M1xUuaHh4eHh4eW8S2ti9xHOPCpQsIlMHXbll64rLb67qdX6/bl+vSoXMBYEyyWjTq\nvMtoKImkJOQAZZFaKxW3EwltjjxjA2sVq73wQwZqVzaQ0IqObPm1NNkT5wMj0oR2TLF5FHPjuHrk\niuQepJwj0z1XuVTKg/53CmMMjDFDElke4G/Wu3lbvkrbf1B5SK1UHKtA4g3uyHUqxw9L3GDvlyip\n0sgYaC5YkvtU8lyjysFEJF8bZK9DSNbu7Dba7W50Lo/naDJAGAApgEzx2fYlO0lsXVC0y33eJ+Js\nph7BEl+QzH0toXYsH2ceHK5c9WUORuK0s3uP23HbjDvLy25XXG2IM1aV+3uh63iAc0kgl3qVw4WM\nnXUSipSTUGzJJGSso0j194j32c2p3fjqb/shIFWB/pfYEacj/Mj9tgv0X5rjsvYCB7zPXzidl3Va\n7DQ1Nj7B13UcD+nKPJOhtFd4Po4fcJlMWi2WaOeWeVz27HJOQmVxBLKSCeBycxohj0DsJMZKxM6I\nxRkOOSnvdUQJhSaHZlEszkJzjjDBrPJa2ZdMIQstJ732evGmIRDbgjEwWTqUwzInv5D3PlVZR4wc\ns0t+OuToKOufzAlVJUicI7M8b7LKKiPOmZNNLtuzeyIva4zzdyBWznGxaDZrJbmfcmJsiXNpqzVM\nRgMApT0Ssij5T6GcEy1vdFsk4oHSQLZ6PZ/lxMPDw8PDYxTYlkjU6/dx7MnjQ/Ywa6eKJTecpjiz\n55Ul36EOjLc2L+ver1mLbCCvDbXodZ3bPboiMYbr7W/W3plqKUXon2yQfjQUgM7nW8lHB8ZaYcO2\nRUs3ayWEYXKD0e9DNgrNsGkehoJ5bRbyxAasa1jeQukbJd0EuW3CSq/uKiuJhNLfE01HZGCJCDpt\nt6Mvi3RbEPvSQEm2tv7N7AXr7avut5VUdX+kaYpNEhJsC4YISRjkoUQAYOTe1q6nNSnG2m2Egq6h\nMyUMhu3KOji9LqQdLcmdqJ8nE42Itfk7jQdgZCtfH3f3KUkO2EQkWUtMAAADyVBBNhuLovkriyHe\njkVB5SS1bbchBRm0/RsjRZYm6C0vDmVO2n+Uw6rKNq/u0PSXsZF+GvSdRBZLQs3YapBSNffkd1sy\n/0xOO2nSZqKJRLopq74oyJqis4K4F0Qk8cC9/wWhHEyL3PaWIlgwiyLpSA5IgnvmgYS79Ikl7kFN\n2XGxChOMRntFRAiCAKGivLOhUlZLpENHUvvca0kO4OYOyfzVOVFTS8QhGo1Gzc29mQafeGCGpcpC\nza0pLdEMdpXfTEuo9cZqluRArdNC9FEVTaXOMEWy1i9J9qFW22kzWj3WLiwI0cSCCmvsxhkGm2Sq\n8pKmh4eHh4fHFuE/mh4eHh4eHlvENh2BEpy/ODekXrOCsjUCF5SqqSKOPDmzxwZ1hiLTx8pIa39X\nSuLkUFbqElExWTXRsLpVGPnVsUwYh6zqRWebsI8RCbuQdsiwbbAOKVqFZtWFtkz3RxRF6xxXdoqN\n6suPaFWndUyx2UqUipPyUBNhYIrc0Nu67PNoDXOeiUOci3Q/WPaNYW2rjOdgvZPLWrak4bLLq243\nCkN5PmCMQS9OkAyxgVjGKn7uIUcqeRwbJjLQIT4y3xLrCNR389tmirCJxEvKCcc+ouWZbfWcmiq0\nHLdFp4K1CbOtw5YNuwCAntzT9ltF3ScfR6sO185rkgh7IEnJI0X+qtW4o0Ay6OPS7DPoKD7rRod/\n24wy+p4FWRMi+7fiVJyVMVa55uuAUuPZ+W5ZwEgz1Nh3w+oXN2C9GcSuD3L1pZzfUuev2PNk3E3i\nrssSebGE01c7VKYZj7NV9KrboZv1hlSmO0UQkPP+wfpwNO0E43LPy3ugVPzW9GLDqzLFjGaZgKoS\nzzdecu+GZfQZq7FK9fyicpI7y6FHUeTmuDUvQLjLVRPyeVsocZ2pCrlaXJVMPjIGK4pfdn5ZnMyk\nyYmKG+xn6aZBg17S9PDw8PDw2CJoO7t3IroE4OTz15xNcQTAAMDZK5x3LeCwMWbXlU/bHC9gf5cA\nXC9/zwC4uPnp1xyu9f6+A8AJAKtXOO/FhGu9z7eCfeA5/+xlym8DcArXxri9FPp7J/gKAI8AGDH5\n8WVx2f7e1kfzaoKIfgvAaWPMe652W15qIKLfALBijPnhq92WlyKI6ASAdxtjPna12+LhQET3AbjB\nGPPtV7stLwU8n/OcWJ9+1BhzfNR1bxdePesBAIcBPLpRAWmmA4+rBiIaTcyBh8dVwEtp/l6zH00i\negURfYGIVonoAwDKquwfEdFxIlogov9BRPtU2dcS0TEiWiai/5eI/oaI3n1VHuJFACL6OIA3AXgf\nEbWI6HeJ6D8T0Z8RURvAm4hojIh+m4guEdFJInoPSVAXEYVE9PNENEdEzxLRPyEi81J6SUaEu4jo\nSzIvP0DE3iBXmMuGiL6fiJ4C8BQxfoGILhLRChE9TES3y7klIvoPRHSKiC4Q0a8QUeUybfmyAxH9\nOBGdkfXkGBF9tRQVZW6vEtGjRPRKdc0JInqL/L6PiD4oY7cqa9PLr8rDXIMgovcDOATgj2Ud+TGZ\nv99DRKcAfJyI3khEp9dcp/s4JKKfJKKnpY8fIKKDG9zrXiKaJaI3vhDPtg6Wqu1a+gegCNa7/zCA\nAoB3glPp/TSANwOYA3A32B7xnwB8Qq6bBrAC4B1gz+AflOvefbWf6Vr+B+CvbR8B+C0AywBeB95U\nlQH8NoAPA2iAbctPAvgeOf97ATwG4ACACQAfAztKR1f7ua6Vf2B75ufANrRJAI9Lv112Lst1BsBH\n5ZoKgK8D8ACAcbCb8i0A9sq5vwDgf8i5DQB/DOC9V/vZr4V/AG4CMAtgn/z/CNiGfx+AHoBvAOd0\neC+Az64Zt7fI7/tkLXmnrEk/AraFFq72810r/9b01xGZv78NoCbz941gE9vlrvlRAA/LeBGAlwOY\nkjID4AYAb5WxfPXVes5rVdJ8DXhi/qIxJjbGfBDA56Xs2wD8pjHmC8aYPoCfAPBaIjoCnvyPGmP+\n0BiTAPhlAOfX1e5xJXzYGPMpwzEiMYC/D+AnjDGrxpgTAH4ewHfIud8C4JeMMaeNMYsAfu6qtPja\nxy8bY84aYxbAH7S7sPlctnivMWbBGNMFj0UDwM1gf4THjTHniGMm/g8APyznrgL4WfC4eXAkRwnA\nrURUMMacMMY8LWWfNMb8mWEi5feDF+rL4QFjzAeNMTGA/wjeUL7meW35ix/3GWPaMn+vhHcDeI8x\n5phhPGSMmVfl7wLw/wH4emPM556X1m4B1+pHcx+AM0a2GIKTqiz3/jLGtADMA9gvZbOqzAAYUgd4\nbAmz6vc0eAOjPe5OgvsbWNPna357OOjNWwdAHZvPZQs9nz8O4H0A/h8AF4noV4moCWAXgCqAB4ho\niYiWAPyFHP+yh2HnkR8CS4sXiej3lRp87biUNzEt6LHIwGvLvsuc68HYznpwEMDTm5T/EIA/MMY8\nsrMm7QzX6kfzHID9RENR/TYtwVmw4woAgIhqAKbAoRLnwGpCW0b6/x5bht6szIElnMPq2CFwfwNr\n+hw88T22hs3mssWQe7sx5peNMV8B4FYAN4JVWnMAugBuM8aMy78xY0wdHgAAY8zvGmPuBfe3AfBv\nn0M1+dwWm/4BvDhC4F4obBSKoY+1wZs7ALmTod7YzYLV5pfDuwC8nYh+cCeN3Cmu1Y/mZ8BsyD9A\nRAUiegeAV0vZ7wH4biK6i4hKYDXU34na8E8B3EFEb5fd4vcD2LO+eo+tQtRWfwDgZ4ioQUSHAfwz\nAL8jp/wBgB8kov1ENA7gx69SU1+M2GwurwMRvYqI7iGiAngB6gHIROr5NQC/QEQzcu5+Ivq6F+Qp\nrnEQ0U1E9Gbp4x54g/Fc0ml/BRG9Q9aWHwLHDH52hE19seMCgJdtUv4kWJL/RpnD7wGrzS1+HcC/\nIaKj4vR2JxFNqfKzAL4avN7841E3fqu4Jj+axpgB2JnnuwAsAPhWAH8oZR8D8H8B+BBYyrkeYrsx\nxsyBdyP/DqzmuhXA/XjhAmJfqvin4EX6GQCfBPC7AH5Tyn4NwEcAfAnAgwD+DLzhSddX46Gx2Vy+\nDJrg/l4Eq3XnAfx7KftxAMcBfJaIVsAOWTc9Py1/0aEEtrXPgdWxM2D78XbxYfBatAi26b9D7Jse\njPcCeI+YB965ttAYswzg+8AfxzPgNUWbz/4jeBP+EbBD52+AHYh0HafAH85/QVcpKuJFQ27wXCAq\nlNMAvs0Y8z+vdnu+HEBEXw/gV4wxh694sofHiwTkiRA8BNekpLkTENHXEdG4qGJ+Euy67FUozxOI\nqEJE30BEERHtB/B/A/ijq90uDw8Pj+cDL7mPJoDXgj2w5gB8E4C3b9Hd2eO5gQD8K7DK6kFwDOJP\nXdUWeXh4eDxPeEmrZz08PDw8PEaJl6Kk6eHh4eHh8bxgW/yg09PT5vDhjf07bKLgTCXwzRMeh/Jt\nVklhSZIVGxvGowReG54ZD2wiaFfW6/Gx7ipn66nWc0paNMbG5D4q+fKadmrJ2iZF7nRYe3vh/IW8\nrD/oDbVlSCLPm7y+7QYGcZwgTdMdZ42dmpoyhw4dwkbagI2O0RaSX2+mWdjs+hf6uu3UMTs7i/n5\n+R33N+nMxM/t+suW6WfYcE69+DBnRpCqqlqrm7GJSQyH812+H7c3yLThzx3WtA6bj+LWnmtzGCwt\nzKPTbu18TZmeNocPH1lXPwCXfF3N42DNnNZSli0y6y9zNcvanekE9GtOzK7wHthk4rYtG3aCWV+X\nWftX3SaVZ42TdF2TCmGIM7Mnsbiw8ZqyrY/mkSNHcP/9nx9q9tnTHIf9kb/4BDcszGNX8dq77wAA\n7N87AQBozExs53boLPOHcWm1lR/72Ec+CQA4/egxAMAb3+pYrO55w70AgKDoPqSbzbJetw3g/2/v\ny4LkOJPzvurqu+eewczgPkmQILkgueQutdxdirocVnhPyVqFD/lQ2A6FZT84ZPvRIYdfbEU4rBc7\nFHqwtLak1Sp8yDp3qV2KS/AmSIIgQBLENQAGwNx3313lh8yszOquafYADUqy/+8BPai/jr/+uv7M\n/PJL4Po1Yj1/61u/F7V997k/BQAEQYN/NYOixdXcZVmrFcTaLl68+nGn1hMOHjyIU6dORRMSoPOF\na1+8qdTHOw6StpN99vLS73U7WRYE26fDdTte0nZJ62cyGTz99NPb7men8H2/54+5YKfr/0Wh1wlM\nLx/zZrPZl5qMI6Nj+Pl//i8T7yuzIPqz/SUeX62tLdQCPV7K/9jtu8HuO3oG+f9B0sci4Xm5U4Rh\niF//j//urvcDAPv3H8Rzz5+KPV+5XBYAkM3R58Azr5GU9J/XbxmjqNGiv32fxjaX0Q2rNXpvXrtC\n79aXX1MuZqZA7+f7jt8HANi7ezJqGxsdoXUyGdNrHmA+dmj6Lu+8Jn/8NreUvtIK+Tql+DOX0vth\no0L9m5tfpz5MD0Vto4NZ/MjTn8V2cO5ZBwcHBweHHuE+mg4ODg4ODj1ixzUPKWaprsq5a1cAANcu\nXAYADE3sjto2N1YBAN6+Cd5WzWpxWzTqLKhh3B9pNs2zRRKDyJSrUVuWezyyexAAsGtaVfI8n7YL\nWtq/dpeNdZeIp6FUIonO48dVQOWjS1QgfGlxjrfTfVaqVe47CQ2JawAg96yX6p+7bjv3TpJLUNbt\n5i7sxQXbzd3aK5L2JcvEpWLdyfK33CP2vMX9I27xeFy6v3HB7c7zTl2wf5nil93umb8ohOjiwk8I\nloVdQhB6LhL7MufGxwh6uI7xVeQ/9p5re6eY/2r8rD2ilngkuxfe7mO7d1fwPHrWqlV9pzYa9A7O\n1en9WcyrazTNLlcvpBCRPIsA0GS/9M2bFKK7fPly1PbmG6cBAGdOnwEAzC9rsZLPffELAICllWUA\nwIjhpUztojD57t36HRkbJyW9sZExAEAmm9UTElqJvG/MeKclFprmZ9D0vcS33OQovfsLGf0UVioV\nhMH2F8JZmg4ODg4ODj1ix5YmwhAIlJgyPkxf6l0TZPltVteitlyBdl8YJnJQ01iAGysUgG0yG7be\n0n2OjFMwuDRAluboxHDU9vkvkm57eZ2Os/egFtXweBbaq03g+9S/gUE6hxMPPRi13bxNFYOuXCWL\nM5fVWcrlK1TtxvdourKyuhK1VcoVpLz+zUWk8Kngjkk7EQOts2/tpJ0k60MswSTLMcnyk9VSJvie\nTtN412pkoa+vr0dtW1tEyiqXywCAmRnlmUxMkKfiscceSzjXjkV3hTAMd2yR7XT99rY7teKTtr1T\nlu5fuMWZdE493Ou9ov252Sn7PL66WMCdbQl77bWH2+6hn/A8D9lsBqlUKVom2QONZoN7oJamz89v\nk9tu3NBKX6+8+Q4A4NSLLwIAzp0/H7WtrdL7OZcmPfZdU1NRm7xvSyXqA7GnuS9V+kac/1CtVnDp\n0xITiHZNqIb70aNUFKU0QPvMps0nTW4jzt5oGI9Gq0nvIN+n785G2UoIezHCUzucpeng4ODg4NAj\ndmZphiHQasQ+tRO7qQbryUcfAgDUTHzv8BHK6fT5619ZUyu0tUaWhcwQ/IbOAmqrlGqSy9K+rLWy\n5wBblgHXfjXxQ8mb9NBpDSXNIsV6yvMMZt9+rf37yCOULjO/RL7469d1hlWpU18nx6nv1arSnMMg\n7GtM0/O8xFSSXijtNv4Q+f4TZlCSr+q1zaDpz7YZsBlHmYXa9SVPV7wKm5uaLnTu3DkAwOuvU9H1\nmze1/u/cPMWOB0vklVheWozaRscoVenfP0AlEEtFTWvyvDup8HTvcadx5bvd9189hPfAyr2z/fXC\nCYivz3/8FbocYRii2Wxp7jyAgQF6nnKcctJoqNfvg4/I0/bKS2RNvnjq5ajt3XMfAKAYIO1HrddR\nth5LeVpWMm1yebI5eu9OTul7t1Qs2VVi+y9vEkfm9vx81DbOXqiBIUoZ8cx5SRxb3sethn6bhI8S\n8JFSvsZJ05kivC7eQmdpOjg4ODg49Aj30XRwcHBwcOgROyQChQiDFmBM2dwQkXY+9ZnHAQCepy7B\nApvkQjBJGyLQELvYsoP0W8zrPusVIoN4rOjhpWxxbzbcIxdoTNhpR2ejbk/6LRi33/T0Lt4jtd24\nrq7EZopM+60tchekjFpQLrtzRZmPQ6/pJTLO3clCofmXt+Pr4qVo+5xRVJI0j2qNKOrG+x4ReqwL\ndnaW6OdCpLp48VLUduYM0c9v3boFAKjU1Q0kkouTY0T6srT3NU5d+ujSBQDASXadA4AXhugneWIn\n164XF2zStehGuLpb9Jou1I2M9EmTgjzca/qLRRdC1w739MlQd/qLIAhRrlYwNKC1nWtlCpWdeYfc\nrS+98krUdvr0mwCAWQ5PmVc4iuxKHR6mb4BnwkgZJlnK+0PSWgAl/W1sEBFwZWXJtFFoLm0UgfI5\nev9L6snklCoIZYsUIttiQilg5froJ92UUJb2L5vO83GonzYE1moFXa1JZ2k6ODg4ODj0iJ2nnKRS\nMYq1WJZ5pg97lnzC3+SQZxtpmwU8yFaofOGNZeFnyeJLpbO8T+2mSGqHUaDWzvOSUiBk5sFWq11b\nKMmICwIDwNrKAgBgdYkJKkyTBoB6i/Y1e41mX0MmOTeb1RlcP+B53o4F26NZU0LSdZDAXkgxUWuZ\nE5BnZt6L2uY56H79BmlIbpmk6IUFGqM1Q/BaXSWrsMyzSQniA0CNg++SejI2MKJtbLXmCzSr3LNb\nKervvPsuAOCb3/wmAOBf/dIvRW2H9h/4S0mMkesjM+0kSzOd3v7x26m1d6ciCveSsNQTQg9e6MWS\n0j0hqwn3zGpafqxmAAAgAElEQVShtpPsYuke7dsZkZM2MfJUgl2phsj2Hhq7j2isbVPClh17klSV\nhLbkIe+fLd4KWljf3MTbp9+Ilp164QUAwJmz9JwtLS9HbfIuKbJFl8moR1CsThF3Ca0GN1t8ab54\n9r6Ud8IWe6i2ttRTVWeBBftstOvU27ZCgd43w1ysY3BoMGqL0t48Fm8wHrSIXJqgQVGvVeMC821w\nlqaDg4ODg0OP2JGlGYJnDEbcwEtnuI1jbLGEet5OvviGDhwW2LL0vdg6AJCq8T4a7KdOm297h4Vp\nrMq26iOAmZmyRZwoFMD/v3r5g6jt27/72wCAN0+/BQAYn9wTtU2Mkm999zjRnDc3VdwAnh+jc98r\nyAywm2WRVCZHPAF2u7PvkWX5Iicpv/baa1HbEqfcrHH8oSrXJLYPvXjj40Q1z/E1M9lCuO8+SkSW\nWeHM9dmo7caNq7T/TZrlri5rmbaFRbJ2z79/nte9EbUdOXgolmJ0r5EU+2u/j+g/dOIiyGAlGq9c\nIenJtzheFJN9lIoM3vYpRckCE9GSjvXUyDJtkbnD2/dsVfY5Xg/1HkVoP+fYpD/e1k3owYt5nNos\n/qTYb1QVo3sMWNK2Eq97D+iaqpJYX6t/UdN6vYGr12bxG9/8rWjZ7LWrAIACy5YODqoHSIeL0zZa\n2hep7iTevHhcsMnL5BOj5yUSfutc3rFer5vt2CtprEn5W1LobHxUtpU46dCWWpqDg/R3Plfq6Hux\nUIwdz/Y9k8l09bI4S9PBwcHBwaFHuI+mg4ODg4NDj9ihIhCRGFp1JYNkcmTmitvPKimox4hdQHXD\nV+bgLKTihXHBhFzVpFojgkl+r6lkkpEALu/TuGID1hMMm8aFyNqHKaYtN5ra96BFfbh+lXROv/Wt\n/x61vfX22wCAYpbM9EcfVvfswBClo1yfIfein1btxFQqE2na9gu9EoHaYb1aAbucZmfJtfnqq0or\nP/XiS9xGqSDzTPABgLU1CrSLGyNbVCLALq5IYAvGimu6lKNl+/cZtQ9WfxJloNvzepyJCRrDFLvR\nLJ/s2DFy6z7wAKlOFU1qULPZVALIXcLzvK5arknrxxfEXTwA8A/+/s8BAE48pGky+/btAwD81m+T\ni+xXf/VXo7bNja1tj9HNVdtV+QpCSjLbSSWQTs5Y19G8J+koO+C5dFPCkmXdiFayVZK1IG7XMMFv\nat137SQvq1jWi5e7W6pKYvH1sH8u8WarhZW1DdikikGuHhJwsftWy76M+b5Kdfa6va9JhbqT0qtk\n3IRAePPWzagty0QjCeEAQKFAbmO5nrmcpiDKcyb3c6WsxENx3aY8Ci2lfd1uaIj2v2vXeMe5NJvN\nrvejszQdHBwcHBx6xM6IQGGIZqOJGosPAPpBTudoNiA1LQGddbVYy7C1tBG1ZcCK+k3+bptZcBjQ\nbKGeol9vQi3HYoZmCwEHmus1o/vaICsybOn6OSPEAAApI76wvk6WzoULH3AXdDhGRsmK2tggDdRq\nVclPjYDOv8yW8wRrowpimq93iTAME2ef0aza1p1sq0TimxpxZ987CwD4zd+ktI0LFy5EbSvLRGTa\n3KzE9g0ApQEiO/lM7CkNqpU3NU1pITatZIMrlxRHh3hf2veXXyaikQTtj96vVWVkjrq5RkSgXWZM\na6z1e/LkSQDA4cNHorYgDPuWXS4VZXaaahFZe2aZ6HDOL5DIQ/6yEhROPPgwAOAf/5Nf4O117vpv\nf/mXAQBp8dxY3WGxstr6DCSkYpg20dc8tEdrFJbrdM3ml2i2bzfvfv73QCc27F1/ttt6YsE0m0JC\n0bFrJ+0kjaEMQtIh7DMofwuhRSwhOs7H2yG9nGnsPPs45J7nIZXJIZPXPjc5TSzid4YJ58CnHyMX\nttXHjdfA5XdP0EnOFCxzasvw4FC0bGyMrF65hoBamFElE+PZEo3rCldHKq/qt2mF63UKEciDvpdz\n/L26//77AGglJVrR6+q9cpamg4ODg4NDj9ihpRmgVquiYbTU0hwXBIUT4Wc0gVRmdU22CsvGIsks\n0Ay31RSftH7Zq2VOcxiltgP3HzR94OoZbJE0qxoDymU7JZFEGCFIqO7RYnmlgUHyb3/qpNZrXFgk\n6+vaNTq/Sk1nPmMlnrkf3N1xvFq1/skkhYs1aY/N4gHSn3c4LgsAv/5ffwMAcPkyydrZyu0b65Rc\nnGYrvFhSIYcBFq0QualWqFb87Mw1WmaCZWOjo7wvunaXL2ldvDRXUD/52AkAwOCQymFVqzRDTKdo\nvCentE7q4YPHAADPfOFZAEA+p9ZuEKT6avtsV7/Utre3RXE2M5tOp2ks9+6n8/DSOsuVWM4kz25/\n7u/83ajtz5//PgDgxVOnaDsTz5LZt58QO5VKO5mMja/xtgH9fvGRB6K2zQaN9//4/uu8MrbFvZTV\nC0GWW6+x4/Yxj6c5tGK/3eKeibFiGVf7rugiQdjPcekaq0X/jE3P85D1vVg6TkNif0m5MO3LuiT9\nx58N/o2sUNMH/pW0jwOH9P0+xGkivokTf/D++wCAd1mGU2p7Wsg1P3zsWLTszLuUSjcxMcXHU2/P\nyAil1VyZoffhE59+Mmo7fPhw12vrLE0HBwcHB4ce4T6aDg4ODg4OPWJH7tkgCFCpVJAxriYJ8Daq\nnO5hdpkRqjBThJuj6vab+YCIKWiQe8nPKWFnfoHcfn6K3Fe7m+oS8Otkmm8sk0pMPq80Yo8pxWmj\n/yrEpFZALoja1nrUtr5BbrIMV1iZmlK90ylW0vd9cokdOqzuQglMN7hwdsMUN015fmLR6DtFexqE\n/F1nF8V5dl0AwMsvU4FYoVpfuHwlart5k9JJhB5fq6mbNc+kgBEOyLcr/gPAMiv0hKEhRHEfxNVB\n7eTWWJaxNSSJEyeJACPjPn9bqeZ7dnMgvkXrn3z0U1HbX/+JLwNQV3ErpqDT33nfx7nWu7abpq0t\ncn9urJEbvIWKWY/GdHiE3EVFU3HiHz794wCA2k0KP6yH+qwdYwLUT/7MV6JlRw8cAAD8zre/DQD4\n3d/5zaitxupN0udLc6op+oUHSaGo5JMqUdmofKXayCz3tAJK2OmeVWKVqIx13v/d+iPLrHKMkPPk\n3o4R6KJ0Odp307ggI+JQQspEkks1ibTX0T/xmicMZWJaUYi+qQK1mg1sLM4h1dKxCflvUYWKXW+0\nuWcTcpPUvb39cWMhM/67VODQj3n3B9IHM45vv3kaAPBHf/gHAIChIf2OZLO0rVyTfFGLXZf5vfzI\nY08AAA4cUAJhk9MSg4BCU8ucWgcArYsfoWbCVx3nsm2Lg4ODg4ODQww7sjRbrQDrG5tRQiigggI1\nTiNoGgEDv0Iz6KFxsiK8kpKEVgo0o9jcYtrxkAoENMcpQDy2R2pa6hSmzoSRVo1mCNkRTYLNFMlS\nSqeN9cmWlceW5saazrbLXEdugNMqGobmXCpRH2p1aivkte+haFRCkpt1GFOpT6ae5lvvEMnn+R/8\nIFomgfJ1TvvIG0JPjmdzEkQvlXRGFrbiCciWJCTVR2RsCgWdFYqWsJ0dS23NKhOGDh08oG1btN+N\nVRr3kskG2jtF1//4M88AAD73+R+J2gpcnaAVaQt//Gz+k4Ren06CzocfUDrTxqZWgin6NF7zj5A1\nnTbknZVLREz42w9TfdqSp1boIFvjP/mzX42Wycz6mWc/BwA4+dD9Udu/4fSVlTUitU0PaRqPd5nS\nrYY4hatc60wJ+GQQdpCvopYEC6Y9ZcRuJ1aH/MboM9GumEBkrOkKv7uErJU13isRPLD3nJDiEsUT\nerAIo+IoiTKznWlFHdq8d4FqpYJz751FuayVRUI+N7kDbMpctxq90paKUt10TKXySSph/OTsklLz\npLqIl/CIyzvYipsIpKqS1XHOyf3Az+KQqVRVrtD6IX8flpbUA/nB++9jsxwXGbFwlqaDg4ODg0OP\n2FlMMwxRrtRi1PqwJX59mhrUKzqDCfhrneIv/cCgUn6PnSRJtFtck9JOrSYOUDxxaj9J15UGdGax\ncpuSxXMchywYC9VP83oxxnQ8HaDRqEVt6WhmSftqbHYm1GptSp0VSVwww7+tVJyi7iUkmt8N7OxO\nLDmpMXnm3FntF1ssRa4pJ3FMAFHVcrHWrPSdzODWWOTAtsmsXVIaslltm59f6ehfnmOY5XXyCCzO\nabWS+gbFDR55gBKKnzipVtGnTn4aALBnP6VFBGk9TouvXQqdM9N7mQ4B7CyGZj0iGxt0nV59jeQK\nmw2NaU6NkndkdZFSq4oFfQyXs2ShZ3nZym31jGTO0/j92XdeipZN7qO0J6ngMDyukpP7DtE4D62S\nQMfT00rtXxHLl58Br55Q9SOpEkifvSghKEE9ttvIk0PnlDZze4/TZ+QRC20sVmTW+B4dNLFisazq\nHMvPZtWaTPOzscVtaU+9Sj53LFcwy/j5qDU7U+m66eipIIl02JxXW0Aw9r9+yug1m1hcXMTqqno+\nusnhtd/v8VrF8ZiuTWMJODUw4PvSihXIO6g9lkzrs/WKTotW7gfxftlji3fMit2McOyzwUIe9aq2\nNZn30QL1a92Mx/LSIlqmv+1wlqaDg4ODg0OPcB9NBwcHBweHHrEj92y1WsWFDz/C6LAGVD02w0NO\nCxFTmBay2b8k2oHqnt13iFyvE1NEKmrUlAKd40B8hskr66vzUdvaAqUpTO4lgolNL1HvgDHtmai0\nub7C56D9y4privtp1XXUZUE7zeYsOSCuSuIb96yfSndQ9u8G7SQJcRdvsf5v1bgqhNwze41SdjZs\nxQx2QQkhSDQeAdV+3FyjYLhN6ZBCrqurok+r+4wUakxAf47dsQUO1u83aTyPnqBr9tnHyAV7xGjI\njo1TWwolPm9beDyQweD/b5+C0A/c6T5t+ouMzSUm9ogWLQAceYAUkf7ZL/4ib6j7+M5/+M8AgAsv\nkFt39959UdvYcXKvPvnsU9GyEpMb1tfpuswtXI/aRobpGkz65LIte3pdF5gIIe7KJDWXZM93f8c7\nCEPUmvW4e7Lt+A3T5vPfMtShEa3OMiEvw77bQlHvy0Ke7mPVQNV9Dg+Ti3ujQmPRMCluHhMb0+Ye\nr3C6wlaNnkFbRDmEkGLkHBIG0ZNC8AkVTaJuxV2k/QpD1Go1zMzMxFyc9nn/ONhnI9WmdZ2UNhS9\nRxO0dOW9Yd3DWji9UwlL0t/s+pEKFK8jGrQAMLGHiG+bHBZaX1uK2uQ9JsTQpYXFqG3x9lU0G3pN\n2+EsTQcHBwcHhx6xI0tzc2MDL/zgz1Gr6ddciCEnjhOp44BJMcgXyDqrbFJ1k5VFJYWUBmkWUGTl\n+lJJLUaZUawucxWSM6eituFBtkLzdLxaRSuneDwH8DOay9BqVrgPSikWZHIU3E8x6SRtZphCGMpm\npIab7lMmiKJn27QzUy/o62Tc8+I8gBxbk3v2ktiCnSVKQm6tIpazWscNrgAjMzlLl5fUmzqTVQYN\nYSudkZkfHWdjXWdgKdaXXVzUWdrICF3XfXsPAwCOHz8atT32GFmWUmNzaERrbWbytF1TLMzYJLyN\ncGBakmpg9hPd6lW2L4vPwulvSbBfXVGiwRzXEV1Zo3u3aFIcTv69nwEADH6O0lEe/8wTUVuGU2+a\nFbUSqiwqMsNCFnMzqvXrcdL66ffPAwDqLfWy5LN0HzVYr9lL0PNMUjztN/EqDAPU6vVk8on8ps3c\nng0+IZ0Uizp2uRGyrPOcLlYa13S0BlvU4sXKGSLQAL+DplN0P5fX9J2yMkfXKjB62yVO5UqxlnK1\nqe+WrbKka20/TlLL0lpTqY5KOXHLLEk7+07g+z4Gh4ZiaWWShtbLpU26TknPRuR1kdqjtu5xGE9j\nSXUhRAH6rhIvgbWS2yv/2DZRpT7z1lv8f/UW5Pkbce5t0l6e4zrDAFAqhmg5S9PBwcHBweHusSNL\nEx7gZ4DX33wtWrTGM+j5W5QK8lNf/3LUVsjTzC3F2bmtqs7Ibi3RDC7NEm41E2tcWybfc0Uk75o6\newg4mXt5jtIpckY2SWYw9bqReuMYa41n5IPDKvmW4tlGoyXxWJ1dVDhmOMLiCTkTOxXLUpPt9Xie\nl0qcLd0xvDCStwKAFs9zRriayIiJL9/mdByfYzs2PaRej8df1tb1WpTZEyBt1pqYn6d4cpqrxeSH\ndPa+xJ6D0oBa4UPDFB+9xddnaU0T6hsBWZ+1FllMmYKmRzTZavU4PpxKqF8YCXrdQ8uyvZ6mVp6X\nY3ffVv+Ot9mZ9vlz5wAANzjdKl/Ue+s9lkU8fIDil5m0pjpsrVLaxNqKpqFU+Jq99vqrtL7JypEh\nrPNM/sqa4Rv41LhV65xRdzvXfg99GISo1eN9kPtQYlmZnEn3yog4AXWk1VKvV4vr8CKgZ/x21fAk\nuK5uYYrGMzTeoSrXka206DnwW9qWZyu3FZo0O5aSTMt7zaQnbG3RNUqS0hTrTp7hYkk9OtG4Jngt\nEFo5u7uD53lIZ9MwRUTQ5PdrJKIQq+Ea/8MKGESGMscmoxqa9L/Ydq2mle3j8eL3dWieddmnfec9\n/uQTsWUXLnwYtd28cT22j4oRJZCYaZ2rPy3M347a8pw2tMY1PTfW9T4aKuo7NQnO0nRwcHBwcOgR\n7qPp4ODg4ODQI3bknvV9HyMjw5FqAwAsLREJ5KVXSaXk+H2qOnL8vkMA1MKvVtQElsoaQY3cu7eu\nXo3aNtgNJfH/sUl14zFTHktcsWF0Wr/72ZxolKoroMUB/MFhcisWjeZqs0FuglV2Va4x8QgABllV\nZ2iY3ItWQcRj6r7n0fYp4+sIwrCPVU5Cdg/q/i5fJqLHd77zJwCA0bGhqG3XFLmlTr9BxVfrJo2n\nyC6HuTlyUUghZEBdr+OsrrSypKpOt26Rm3V6N12DSkVdg4PDNJZPP/1D0bKZa1dpPXYHl0yqxQsv\nUrWCfX+TSC7ZvLZVqnGiUje0V5zoFznF8zx26Wyf0hL7r9euBJRw3YXsYPr4Lqs5PffcdwEAX//K\n16K2LXYX3eJdDWT1ET37Hl3X5777Z9GyX/jFfwoAWOcqDceOaGHvoSEaX1EjahqlqnRb0QrrDvP6\n5ArsBSGAFsKYP1u0X4/sp4LCBUMECiMSIo+9SfdqBXS/j3GR8taG3v8lHgu/TO+upiFFpfO0rNaS\n8IRuJ+krMJWdFtg9fmuFxrzR1P7l+B0kw2mJekIK2+LnMvStylgq9htL34AXaeDeLdKZNCYmd2F5\nSUmZW5t0HgV+x6WMGlc7vATXbcDu6hTUTR2GdN+GAaelhHofi3s2ctOaMQr4mbL34+FjRCY8eJh+\nF016yEs/eB4A8ML3nwOgYTgAKHIVFUnF2zBk0AV24+aY1PXAyUNR28RQBm+8e77z5BnO0nRwcHBw\ncOgRO7I0Pc+Dn/KRyyo54fARIndI8vv1Wxps3bOHrJOwSVbEFWNN7t5N4gZjQmQxwWCfZwtDDZp1\nDO3TmY9foFnkwASJImSsJiSTSQZKelpBgfab52R7m1AshIMaB4ql5iadK80nmk3qQ8aksYilKVRo\nO5tstYI+ElUoncLmQL/88kv8S8SP++7XOp+fOvkgAODEQ6Q5ev6cph9UyjSzlpqh80YTVjgRy6w9\nK7UgAUD4K9EyT2fox/ZTepFcewCYYzLSyDhZ6hc/mona1pZpRnvkEOnlju3S9KTRUSVoAW01Mzus\nvc4E677AA1Jpv21Rm96tKTkRcSKiRZ3U+ahGoyECPfssVXDJcN//5//531GbpJ/8p1/5FQDxpPqr\n10m0ojiiwhQ//bd+FoCmUExMqpjEfUfo3nj+RSZxGVLRGtdHbbdwYgg7k97VAEpKUbkDeB5S6XQs\npavA+sV795Oww2jBVORhoo0QbioVJX7U2LtRWaf3TcpYgBub3FalZ9wKkPhp0ZCmB6FqUuoyzEyx\nWqhlthgbQSc5ZnCwyMuEqKRjKNU51irUvy1jFbUkNWMboYF+EYF838fo8HCMvCTpNFW2vuupTnJY\nUqpa+70TqwSTFj3azk9MRK7j8RP9ckA9B/Y9LURNj1OC9h/Q98aTn/kMAOCVF1+gvpsaqtPT9Czs\n378/1k8AWOPKPxvrRP4aGlYPTauyjFRCBRaBszQdHBwcHBx6hPtoOjg4ODg49IiduWdB5n3G5P9N\nDFBh3DFW36gaJYVLM+RO2lqlvEtrvk/vJjN8i7UCQ6OLuWua9jmRIddtjotYA4CXo2NnhXwTGFed\ndMscJ8eunjS7lBumRJMUVg7Z5ZY2JbEaDdqHlOKy5bKE+COB/ZgShtdP9yy5+cQVBQAfcFHjSpn6\nPjureorpzEfcRm6fKeOqq9do/VHO77Tnc4X1UQMmBA2PaJ5SschuC7k+puC2uLNnjZqG5LA2qvS7\nMKfB9wMHyZV/YYbUazZ/79tR2+MnTwIAjj9AurTDw5oP2o6k8kR9QeghDNuybDsupVnQIR0adKwX\nsnu3lNfydj/+Y+Se/S+/9msAVAELAL7xM6QI9M5HdC0tAU2Kfre21CU5c/kiLeP/T+9Wd/2DD5PG\n7cAAjWVgSzdFrsVO3dBIgSk6V3Nefc+RDRGEAVrmmZVSW++eJTd+xsztfXbl1evs4iyreo8QUpCg\nUCO5y0luTnE5ZkV5pq4uPq/Riu0TAEImJm3x818PtH/i1pMSeda9Lsepszs0sPo/QvQRglZHnmZ/\nELQClDe30DIkQeFSSUlDqyXdriEbK/El70/Zj3F/5iNCFCuqmTCa7CvKczf7lOtkHzNRcWqGnS5v\nv404Zd3OElITne2NDb1X5D0oBFE/rSGAzZVynPDUBmdpOjg4ODg49IgdKgJ5SGcyMSslw0WKMz7N\nrHxfv8M3Ob1haYGsoUceeiBqm95DRCAJjo/tUqtIZnyDE1R9wM6Ct+al4gnPDotGVSNgQo+la/Ps\nqVbjmamx2kQVJypQbZSEZOLn86wyPvvqDHwLwj6qAXkezVwtMWd+fpH7TP25PqOEntkbN2L9O3Dg\nUNQmVTbW1ynVZP9+rZ7B8rrIsdqKb07hsUfJAtxkdabBYR3v8+fJGqoa5ZU8E7U++9QXAAB79qq+\n7ACTJPbtJ4LY5fcvRG3rHJCv8nUa8ZQYdK8LTVu0qzl1HNsKAbepuMSmxympZEHbH39AC25/dJGs\nwxdPEalr925Nqcrl6GIUeGbeNFZoiyn6tsb52BjNlP/X7/8+AODTjz8ctRVFI5WfycBUBEmzBVvl\nW8umTekpy7HvpQITeXwsgUMQafrGNuBlUhvevIs8JgKK7q8dqGbbKVgikLSFvG8/p69FL92pWiPO\nLY+tIj+MW4WAkleaNlWEU9zEFxckFfmOPBT35p5fX1vDn/7RH6OyoSlnayv0fpb7REiTAJDnAvXy\nzpf/22UR2c1YebIv8XA1m52pN1q8WvsX8nrNlu5raY7e+WNT9JzINwcAsvy3pJVY3ex2glLapNII\nL0/SJ2vmHbu2tOKKUDs4ODg4OPQDO7I0U6kU8vkC0kbgMsN07RzHOa2luck0bw4PomY+3n6GZrql\nIfItF4Y0blnf4hSGdbYKl3RWlGGd0/yuXdQnYwG2ajx98HQmImkly5w0bi1NqWTS4DjswrwmzYqF\nObmLU1tMyonMmrJZnmEZayDVasUo6HeDEBRjC03cVqzhQa6juLysMU3R3hUr5cplra04xKk9UhVC\n9DMB4Nj9ROF+4DjN5DwT9z2ydxd3hq6PP7AnapvnJOPFFY0VrHIC8VGubvLXfvwnojbRe9y3hzwI\nt06ctCcLANi9m9p2UuOvX8hkMpieno4ta4/5WWEPicNFMRoruhC1cb1Zk77xve9/DwDw6KOPAgBG\nRnR2vM6iEz/27Odj21NfZPauy777HCV1l8t0X//RH/9J1DbBaVly7UXfFwDq7F7I873SMNUhxOLS\nGJVNaO+/BRS2G1ztFT/iihLcllBZo207z+w4jIcM28LRXK2I/2ct+cjtYi1xXj+yFG3tV+lfhxfC\n9kF6mFBFpmNJf+38eqOBmzdm0TJ1jwOuD9ri90fZVEARa1Lu/6EhFVMRL6GMe87UHK6xpnGL0wYb\nRntW/r59i2oj5319t44Oq1a14Mw77wAAnvjc03Scgmo1D7Bl+ZWvfhVA3LKX/qh+tF51EUFYuk2e\nuoUF/cbUNxfRrG+fUuUsTQcHBwcHhx7hPpoODg4ODg49YseKQJlMOkaKyTBpJ80amVsbavYLQcRj\nZ8jtm7NR240bRBDJsk6kNbnFj1G9RISJ7Mh41FSYou2kpFhoAsbiQmnU1X21xAWSF1kjN210LH3m\nWs9eJ9WahWWl8hc54L2L3cDW9eB55HqQUjhp4wZqtJoR+eiuEYYIwxAFE5gX7VyfxUOzRklF3CXS\n17m5+ajN81glxaMxWlrSVJBKhcbk6CEiY9136FjUtnuSCDnTrP9bHDsctV26Qq7hqzOr0bJ0hsbm\n7HuULjAyrO6czz9FGrVL7NYtmbJuUgg4yS2bRLjS8+qf8yrlpzAwMBBR3AFV2snzmEpRckDvfemC\n7cvgIOte8nldvqzKSAeYhCUpT5ZY9/qbbwMAHnn4EQDAxLiq/8jYfO97WpTdT1O/nn6a3Lm3bt6M\n2kaZav+jz/4wAODs+1eitjK7tYby1Jd6Ve/9+QW6b5ZWyWVlFanC7S/FXcCLE1+6FTduu9723ugs\narwzV3Kk+5rUaNN12vqHmBu4vTB5wq68nfWrn6SgoNVCeWsNzaamBmb8uKvelgYr8TtF3LK2eLXo\nUm9xCpRNE5O/JWUqDNTducEKYu9z+txzm38ctYli2bGj+g6amKDQUK1G35aGIRV5Pj07J5/4LK1j\nCGWbks7ItuHSkr6nzp0l/WdwiM63KmNhoyv50FmaDg4ODg4OPWJnKSfgNAjzqZXZdjMSA9CZiMxm\nUpwYf+PyuaitypTncpmCu9OGdj/AZJXcNFl5+Slt88WyEt1Cq1HIxxP9UwC4ynq3Q0y2KBaVMl3n\nNAqZ6bpLcO8AABE3SURBVAtFHwDyeU574Rm5neEKKUgIQLYCge/5/dWehYfBAe3Xvr1kGXxwgdI1\nBgb0fA4dIkLP5CQJErz80hvaZ9YdrVZoJra+rhbJF7/wFADgice+CADYO606jPv30sxvgK3C/ICm\nBh09TMnz9fr3o2UT47TtpYskmBArPsskpvERsl4fOnEiamsfZzvTa9dFvVcpKB4APwWkjaegwSkw\ndb636oYgILT0HAtuHD6kmpgDkkrFmrq5E2q9Tk3RGK2yJffyK69EbTUWrbhylYRBxseVJHRk3yEA\nwIhJ+7k+QxbsBx9QVYawbkQCmPT24ImHAABnz2nx3q0y3ftjg9SXk6xbDABrKyvcB7JMr81qWlPA\n4hVra1rt5u4RdrUm71TMottzmLSfsN2C3G5fHZbm9sXHu3dwB+v2CaVSEY8++iiWlpRAKJaieDJs\n6oh4gMSLZYly8rdUTFpZUQ1qERKo1+k4GeOhucUEoJuzsx3H22Ii4c1ZJTFKmsuhi6Sl/aWv/XTU\nJlboVoUsxqatmNJ2eQZMceka6/8GLfrNG3JrZWsjVjS+Hc7SdHBwcHBw6BE7tjSBeL1ISYpeXaUZ\ngk1KTXHMsLxCMZKbH52N2m5epXjlLY5zHjx6X9R2/PhxAMDRo0cAAIWG8WFzRRIvRccJ6mrZrqxQ\nrOz102/p+hxPPfEQzaQzaZ3eNdmibbEww2pF407jHEuSeGLNJPCnOc0mZJJ6q6XxgbDZ6CMtn+pp\n2soChw8for4/R8fcs1dnfusb5LP/2te/TOcwtitqe/75FwEAN7kKTSarM6uvfOUrAICvfpl+ayZu\nEdHx+RoYwxFPfPpJAMDkpB5nlmeRKY6TXJvRMf2zMu33Gz9FM0UbG5fZpizrFse8VzJ6vu9jdHQE\nRWPZZ9ibUL1N93DexB/TObpvCnyuhVWN56c2aaCW56hG66jZZ3aDxmGaYzN/Y1ot1Lkix8lv0gx9\n2Nz7W2vUh0kzCd5kazLDsaPgqKYEzbxO8dHzK7TO6m2NcYPTrLx5Os7SiloeG1U6j+o63U/1NY1/\n17vMwO8IHllx3ay7pLakupOCbrHQKHXI3F+97Mu+8yI5Qkkh8ez7cPuKPNH2ndoLUX80FtopmNAP\n5PN5HD16FPv2qbiJWIrzLByzvm6uN6fsSQ3YAXMfi0dLfo8cOdKx3Sxbk+WyigeIlStjWzB8FrFo\nRb7Ubrv6FtXjHRhS4ZMf/VGSpCyxl9ByDtLyrmfpxa0NjWkW2GMpcc4rF69Fbdl0KkpDTIKzNB0c\nHBwcHHqE+2g6ODg4ODj0iB2nnKTTmZi70BfFDDa1M7aIL0diyxtEGqjX1O3nsTtu9vJ7AIDb1y9G\nbeV1crMOFFjv0FR6KPF24s7YXNPg8/nztK+33ngzWibm+9QUua2sNmx1i9wSN+Y2uO86hxhnNRUh\nGtlUlYa4KpusdWvcs0HQ6rtupHUbPfUUkXZeP/06AODK1UtR2+gokXTeYjfGpx55VNvGyaVRZyWQ\nPXuU0PO5z5GbtVohN0jLEJsCHq+A51eB8c9OTVEQ/hvf+Fq07Pf/8A8AAAsL5JYcGtSUkwcfJBe5\nBOuvXzOKRQNEbhGFjyQXbC8pCHeDIAiwvrGF4VFNcTpw7BAAILP/IABg/l0lsxUKTFhjdauCcdOJ\ny1ZcUXnzqBU47SfLz45vqgbl2P05eYCqlWTNvbTB1WuQ1hDI0D7ql6SvlFc1deTYYUoPemeJ7u/P\njmr6SlkIEHytS765Z8WtxeSv4aymPNVadP1f2FD32V0hpBCEvd7tLspEjecubvkkF2z7fWLdrYku\n0S7Hi1JbOJSQlIaT1IcIfqet0n4+drswCPsWhgg5jc2+w4VMM8IEvdVVdWNev07PqFQF2TIVdoRk\nKaknsj2gLtu9rD1tjyf7kGdD1NoAdctaN7e4XiucSnj6jVejtmtX6Luxh7XM9x3QKj9D3J+QQyyX\nPlSt6yuXiVS0uETvqc11PeepXeOxykPtcJamg4ODg4NDj9ixpen76XjgW+rH5Wm23LRWF1sUdS6l\n0DJJwL7opPpiORoShUdtg0M00xVLFQBuzxKxZJV1aS9e1NnD++fI0mwYi3ZkiCwXsZBaZsK2wbPl\nCltYklgLaEBa0gpimqM825Z0ClsBAWEfi98BgAeEhpJ932EijfzrX/oXAIBv/vZ/i9pk5tbgagqn\nXnkxaitysv3DD1H6weOf+XTUli9SID6qsWgOn4IkjHObsUIlSfvrX/pStOypJ54AANxmTcdhY2ke\nOEB9F89BUiUBmWFbS6BbKkA/iUDF0gCefOqHUDS1/7J8H0w+SBV6Rid3R20zb5BF3+Q+lM1seo2f\ngxlOzbjMM1oAmJ4QPV++b8w4TDDRYj1LXpBCTq1Q8XpUyvqs+EzEEy+BLarR5OcoxaLPqYaSMTJM\nQlpi63XWJLtLMjjYu+KF6j1SKY3+IESnoIUliAHJnoUkq/BO01HarUK7H+mLXb+9HiTM+ERytNE9\nayxGfpY8Xma9XrK+EOJi59zzWfWGsM00lmNL6p1YnoBamAcPkkfDppUIYUgsU0vekTbZp60+Msoe\nj/37ySq0KSeLLEYjVUsAtVYrVbpHbxnRlsv8/v/wPHmASoNKVBplb+HEGP165h0+f5us4wwLmci7\nCQByGR8Li0qMa4ezNB0cHBwcHHrEHYob6MxKUguGhtjvXDGz2ah+GacRmBmlCARIjUA7ncoXmD7M\nEmEpUyFieZFmGW+dpsT9Sxc0xlRhsYKDB49Gy4Y4MbbOSbPlus6oZVYTsuU8sUtTJzrp7oaizn2V\nWb49r77nK4chPHP+4p/ft5csnn/08z8ftZ3lSvcX2Hd/+t23o7YsWyzn3qfxuj2vCes//SVKNXn8\nQarFaGf+nhenwtsTjGpPGg/Cob0HYr/xU4lbjDbOkTTL1z7ErYt7Jm7gkTRiYFwFabamN9iKHzxg\nkrsDkrp7+TvfBQAsGpmuuWW6tyri9bD3/sxVAECe6fFpT5+nNMc5C1z5IWdqB2Z4imtDYgW+v4vs\nLYBnrF2O07TYvWK9LCHfu02pwmKsu1Bi9iyE0TBpJkl1L+8GHoB0anvLEohncLVXN+k1pt0tpSNo\nq1bTra2z9+19aKuHGXte5P6Xc1F0xOvvkaUZhiEajWZbfWA5QqenSdYbHydrbWxM4+KSViLW59yc\nispIGkulQl4RG7eU9T78sNNmkypZdkjVE0jvPHlvA0CujQNhRQmyzK85xHHOyqbGYy9zTdu9eykW\nOjaiEoBh0MS168q3aIezNB0cHBwcHHqE+2g6ODg4ODj0iB27Z1MpL1b4WYz5AtPTx01Vhvk5cmmM\n7CGliHJFCQwt1r4U12PDMBjqrCNYZ4rx8rIGfsXEXluiZUFVi4cOsMpNCmqiz1wiJX1RAmqZgqer\nG9SHEitMlIzahajiROcao6/rWNj/y999zIKA57WNN++8yWSfMVO09Uee+WEAwJOPEcnn8BGtSFJh\nJaWjR8l1LTRxAPjw/PsAgAwPm5CF5PgWSTT+Xt1Z7fuy/2/Xl+1G1LBIpVJ9SzsJwxDNRg2plJJv\nhJp+7hy5tefnlSAgVW6qrLMpVXMAYGI3Ue6FYHD/iQeitkDcn3Uhp+n9Ku6sTdbGbJhwgpxmxuhk\njk/QceT5W1pQF9nqDKmcNPhebgZKuMhxlaAprkYhmsYAMDBA+6pzod66UUfZZBLSpW99G/2BB9/z\n4tw5cV9G7lK7Ot0n7fcL0On+T0oTSexB232cREJLUhDSXdr7UkIIXRStkIr9JvUhdtJd1LHuBJ7n\nJZ6PsMh8k+6h/Qpl5agtm2VFrAK9U8fG1cW5zipS4pa1BKLVNSYOMRGz3tD7Uh3F2r9FftdHgbKW\nhghy3IfhYXqHlw1JbnOVnsszb50BoC5je6RVDqM0qhpWLBYLLuXEwcHBwcGhH9hxykkqlY7P8uTz\nzzMyWz8xDOOzplxeacTLs5SUX16mgGtgtQnnrgIAzp0hkYIbF89HbZtM9imv0wym0TT0cO7W+opa\npq+++gL1JU/9GjSBbJlLDHKtuBjBhGe0HkRn1hIgePaVEjq6zvyDMOif9Cw0GTl+ZCAlpBgzYxQC\nzxBbzM9+/otR2wrPsqQ+6ING6/f2TbI615ZpNmhr5gn1W/adZAEmVSQRAkE3ndhutRCTtktKdm9P\njL8bhEGAeq2Cmpl1nn6dCGdCWjpyQC0yqT4zwsIRo4ZMMMJ/FwqUsrJ7vxKjDh6hsb9xjSzBudkb\n2gk5f76vq8Y7U63V+FfvRakKMc+VfcplJTsMDFK/8jlOjTKz9yJbplNsEY+bNAO5doUB9hqY+oUj\nCfVO+wFra2rxkDhxhlaMp2YkpYdoKohpE71oXhQXD4hbFZYIJ/djnLQm7XECjd1vN+3kiEnYxUES\nxvrXvzGnd3gqdo7yt77WjaUpKTeRBW0uhhDTODWp6KuGrHhD8iwAkjWpUykmu+VY/7Vi7nFJA2wa\nERXZl9RGbhgymujSSrqLZ+zACntrlqWii+m69HljXTx3us9Go5FY17fttB0cHBwcHBw+DjuyNMnq\nCdr89eLzpy+znZFNTXKlEPZ5Lxd1JjLIsZS1BbJ8Fq+rSMHqPCnjv3mKqPwid0dHE1o0K+TnNUbp\ncd1OpFV2b3CKYni1FK1nLdo9e4jCXOQ6a3Z2I5YcxGJqGWtScr89sYpMTC/0+irt1m5pCrzo1x6b\nadccI8j6ei2mx8mSkHqINiq9bzfRrvdw3dKkGGU3ay5pVi3pODu1NJPSStr3f6+qnIQh0Ki3UDDi\nBs/88DMAgBGOmWSN5F06S+Pre3ErBjDxFx7KGzOzUdvKGt2D+zlhfHJ6b9R2hZO1A37GaibeU+YY\n4+qKPg8LXFVmi9NLcgWVvCuxMEOWE7jtcyv3+u0b1K/VBaXxS2xfntG0Oec+K0TCg1Q5McuiR4/G\ntWEqJ7VbhUnPmixr2XU5RiXJ9rGUjh7i9hYSy47uVbutWCjiOUkSYUjYbrtzAAAvIX57pwhZtrB9\nGaDymSkjXoP2NBxbmiWMe4XssygxQbmGVjhG5PYkxmjTUXK5lVgboJ6vFKdT1djjQsdhAZ16Z1US\nL4p/U58zpkKRpGhJhZVstnfZDmdpOjg4ODg49Aj30XRwcHBwcOgRO3bPNhq1GBvaT8Xdaq2Wfod9\nrpwwMkpunpjSBK+Wz7MraEyL5y7PklrD6q2rtE+TjiLVRtLsIkml1KweGCf34r7DD0fLxnaTC6zC\nJn2xZMkapHLRYpdlyxAexIXRbMV/AXLBAkpGCIJ74y4EOnU4zYE6142o8HINOv2FfuT+1Cbpvxwr\nVnA3QSWlHdYl35FW0oXtYM8tKYWgvS3Jdev7fv9SToIAtUo91ucsu/9XuMh6KqakQn/LuFvXYZP/\nFgIKrKrODXKpXr7wEQDgGFd/AYA0q2G9/y7R5DdN4VzR+Fw1RaHLnJ4lIYJ0SteX1JFB1uO0Lih5\n/hp8rtW6hiaW14iqn+UC2tY9mzcFg/uBENtXOYkIYHYDFXeNrQN06hbb7cK2dZJCENG6Vi+2jZQE\nmOvd9cTCjnWi9BU5riFmReGWpOegS/rKnSAIgmQ937Z+WogbNB6Zi7tn7fOcy+Viv7ZN9iV6tKI2\nBCihR3S0AWCDU7pCjnVUKuqeFbeskIOsjq0UkpZl9h0jJCTRGI89G77f/X20bYuDg4ODg4NDDN4O\nKwMsAJi5d935fwYHwzDc9fGrdYcb757hxvuThxvzTxZuvD9ZbDveO/poOjg4ODg4/P8M5551cHBw\ncHDoEe6j6eDg4ODg0CPcR9PBwcHBwaFHuI+mg4ODg4NDj3AfTQcHBwcHhx7hPpoODg4ODg49wn00\nHRwcHBwceoT7aDo4ODg4OPQI99F0cHBwcHDoEf8XBXQW9WF/5G4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x216 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6JuLDlLstUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_model_history(model_history):\n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
        "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
        "    axs[0].legend(['train', 'val'], loc='best')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
        "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
        "    axs[1].legend(['train', 'val'], loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmcjtnKFszJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(test_x, test_y, model):\n",
        "    result = model.predict(test_x)\n",
        "    predicted_class = np.argmax(result, axis=1)\n",
        "    true_class = np.argmax(test_y, axis=1)\n",
        "    num_correct = np.sum(predicted_class == true_class) \n",
        "    accuracy = float(num_correct)/result.shape[0]\n",
        "    return (accuracy * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMQkBK7pXFhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_img(img):\n",
        "  img = img/255\n",
        "  img = img - norms\n",
        "  img = img/stds\n",
        "  return img\n",
        "\n",
        "def pad_img(img):\n",
        "  return np.pad(img, ((4,4),(4,4),(0,0)), mode='constant',)\n",
        "\n",
        "def random_crop(x, random_crop_size):\n",
        "    w, h = x.shape[0], x.shape[1]\n",
        "    rangew = (w - random_crop_size[0]) // 2\n",
        "    rangeh = (h - random_crop_size[1]) // 2\n",
        "    offsetw = 0 if rangew == 0 else np.random.randint(rangew)\n",
        "    offseth = 0 if rangeh == 0 else np.random.randint(rangeh)\n",
        "    return x[offsetw:offsetw+random_crop_size[0], offseth:offseth+random_crop_size[1],:]\n",
        "\n",
        "\n",
        "def get_random_eraser(p=0.5, s_l=0.05, s_h=0.5, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser\n",
        "\n",
        "cutout_fn = get_random_eraser(p=0.7, s_l=0.1, s_h=0.3, r_1=0.3, r_2=1 / 0.3, pixel_level=True)\n",
        "def reg_fn(x):\n",
        "  x = pad_img(x)\n",
        "  x = random_crop(x,[32,32])\n",
        "  x = cutout_fn(x)\n",
        "  return x\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcg2R92hbmXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import np_utils\n",
        "train_features = np.array(list(map(normalize_img,train_features)))\n",
        "test_features = np.array(list(map(normalize_img,test_features)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOBPTK7LmJJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = np_utils.to_categorical(train_labels, num_classes)\n",
        "test_labels = np_utils.to_categorical(test_labels, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvJrfyNnbeKQ",
        "colab_type": "text"
      },
      "source": [
        "## Resnet Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud7AXd6oRwUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "class StopAtAccValue(Callback):\n",
        "  def __init__(self,validation_iterator, threshold = 0.9, ):\n",
        "    super(StopAtAccValue, self).__init__()\n",
        "    self.threshold = threshold\n",
        "    self.validation_iterator = validation_iterator\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    score = self.model.evaluate_generator(self.validation_iterator,steps=len(self.validation_iterator))\n",
        "    acc = score[1]\n",
        "    if acc >= self.threshold:\n",
        "      self.model.stop_training = True\n",
        "      print(\"Stopping Training:: Val Acc = %.3f Achieved\"%(acc))\n",
        "    else:\n",
        "      print(\"Continue Training:: Val Acc = %.3f, Threshold = %.3f\"%(acc,self.threshold))\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jcFuCSNQgTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow as tf\n",
        "class custom_loss:\n",
        "  \n",
        "  def get_L2_enhanced_loss(model):\n",
        "    conv_layers = [layer for layer in model.layers if type(layer)==Conv2D and layer.trainable_weights[0].shape.as_list()[0]>0]\n",
        "    conv_layers = conv_layers[:-1]\n",
        "    def L2_enhanced():\n",
        "      total_loss = K.variable(0)\n",
        "      for layer in conv_layers:\n",
        "        weights = layer.trainable_weights[0]\n",
        "        total_loss = total_loss + tf.nn.l2_loss(weights)\n",
        "      return total_loss\n",
        "    return L2_enhanced\n",
        "\n",
        "\n",
        "\n",
        "  def get_combined_L2_cross_entropy_loss(model,alpha,batch_size,total_data_size,\n",
        "                                        epochs, end_percentage=0.1, scale=100):\n",
        "    l2_loss = custom_loss.get_L2_enhanced_loss(model, )\n",
        "    iters_per_epoch = int(np.ceil(total_data_size/batch_size))\n",
        "    num_iterations = epochs*iters_per_epoch\n",
        "    mid_cycle_id = int(num_iterations * ((1. - end_percentage)) / float(2))\n",
        "    container ={\"iterations\":0}\n",
        "    sess = K.get_session()\n",
        "\n",
        "\n",
        "\n",
        "    def combined_loss(y_true, y_pred):\n",
        "      container['iterations'] = container['iterations'] + 1\n",
        "      iterations = container['iterations']\n",
        "\n",
        "      # new_alpha = lr_momentum_decider.min_max_scaler(iterations, num_iterations, end_percentage, alpha/scale, alpha, invert = False)\n",
        "\n",
        "      new_alpha = alpha\n",
        "\n",
        "      loss = categorical_crossentropy(y_true, y_pred) + new_alpha * l2_loss()\n",
        "      return loss\n",
        "    return combined_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNzWCeAcaCt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import BatchNormalization, Conv2D, Activation, MaxPool2D\n",
        "from tensorflow.keras.layers import add, Input, Dense, Flatten, GlobalAvgPool2D\n",
        "from tensorflow.keras.initializers import zeros\n",
        "\n",
        "\n",
        "def ResConv(x, kernel=(3, 3), depth=32, maxpool=False,lastresnet=False):\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    if maxpool :\n",
        "        x = MaxPool2D()(x)\n",
        "    x = Conv2D(depth, kernel, padding='same', use_bias=False)(x)\n",
        "    if not lastresnet :\n",
        "      x = Activation('relu')(x)\n",
        "      \n",
        "    return x\n",
        "\n",
        "def ResUnit(x, depth=32, maxpool=False):\n",
        "    x = ResConv(x, depth=depth, maxpool=maxpool)\n",
        "    x = ResConv(x, depth=depth,lastresnet=True)\n",
        "    return x\n",
        "    \n",
        "def ResNetBlock(x, nunit, depth=32, maxpool=False, name=\"Block-1\"):\n",
        "    assert nunit > 0, \"Ensure there are at least 1 unit in the ResNet Block\"\n",
        "    nunit -= 1\n",
        "    if maxpool:\n",
        "        xskip = Conv2D(depth, (1, 1), strides=2, use_bias=False)(x)\n",
        "        xskip = BatchNormalization()(xskip)\n",
        "    else: \n",
        "        xskip = x\n",
        "        xskip = BatchNormalization()(xskip)\n",
        "    x = add([ResUnit(x, depth=depth, maxpool=maxpool), xskip])\n",
        "    x = Activation('relu')(x)\n",
        "    if nunit >= 1:\n",
        "        nunit -= 1\n",
        "        for i in range(nunit):\n",
        "            x = add([ResUnit(x, depth=depth), x])\n",
        "            x = Activation('relu')(x)\n",
        "        x = add([ResUnit(x, depth=depth), x], name=name)\n",
        "        \n",
        "    return x\n",
        "        \n",
        "def ResNet18(x):\n",
        "    x = Conv2D(64, (3, 3), strides=1, padding='same', use_bias=False)(x)\n",
        "    \n",
        "    \n",
        "    nunits   = (2, 2, 2, 2)\n",
        "    maxpools = (False, True, True, True)\n",
        "    depths   = (64, 128, 256, 512)\n",
        "    \n",
        "    for i in range(4):\n",
        "        x = ResNetBlock(x, nunits[i], depth=depths[i], maxpool=maxpools[i], name=\"Block-\"+str(i))\n",
        "    x = GlobalAvgPool2D()(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu6j6REss1IB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "WT_DECAY   = 5e-4\n",
        "LRFNEPOCH  = 4\n",
        "MOMENTUM   = 0.9\n",
        "EPOCHS     = 300\n",
        "LEARNING_RATE = 0.01\n",
        "\n",
        "batch_size = 128\n",
        "alpha = 5e-4\n",
        "epochs = 100\n",
        "cutout_proba = 0.25\n",
        "max_erasures_per_image = 1\n",
        "end_percentage = 0.05"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdivEUWzZhHH",
        "colab_type": "code",
        "outputId": "da3069a0-6b83-43e3-ed49-697918e433e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras import backend as K\n",
        "def model_init(optimizer=None):\n",
        "    xin = Input(shape=(32, 32, 3), name=\"Input\")\n",
        "    x = ResNet18(xin)\n",
        "    x = Dense(num_classes, use_bias=False)(x)\n",
        "    y = Activation('softmax')(x)\n",
        "    model = Model(xin, y)\n",
        "    \n",
        "    if optimizer is None:\n",
        "        optimizer = SGD(lr=0.01, momentum=MOMENTUM, nesterov=True,decay=5e-4)\n",
        "    model.compile(loss=custom_loss.get_combined_L2_cross_entropy_loss(model,alpha,128,train_features.shape[0], epochs=100, end_percentage=end_percentage),\n",
        "               optimizer=optimizer,\n",
        "               metrics=['accuracy'])\n",
        "    print(\"Model Params = \",model.count_params(), \", Metric Names = \",model.metrics_names)\n",
        "    return model\n",
        "\n",
        "model = model_init()\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Params =  11181760 , Metric Names =  ['loss', 'acc']\n",
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input (InputLayer)              [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_297 (Conv2D)             (None, 32, 32, 64)   1728        Input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_263 (BatchN (None, 32, 32, 64)   256         conv2d_297[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_298 (Conv2D)             (None, 32, 32, 64)   36864       batch_normalization_263[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_221 (Activation)     (None, 32, 32, 64)   0           conv2d_298[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_264 (BatchN (None, 32, 32, 64)   256         activation_221[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_299 (Conv2D)             (None, 32, 32, 64)   36864       batch_normalization_264[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_262 (BatchN (None, 32, 32, 64)   256         conv2d_297[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_60 (Add)                    (None, 32, 32, 64)   0           conv2d_299[0][0]                 \n",
            "                                                                 batch_normalization_262[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_222 (Activation)     (None, 32, 32, 64)   0           add_60[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_265 (BatchN (None, 32, 32, 64)   256         activation_222[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_300 (Conv2D)             (None, 32, 32, 64)   36864       batch_normalization_265[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_223 (Activation)     (None, 32, 32, 64)   0           conv2d_300[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_266 (BatchN (None, 32, 32, 64)   256         activation_223[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_301 (Conv2D)             (None, 32, 32, 64)   36864       batch_normalization_266[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Block-0 (Add)                   (None, 32, 32, 64)   0           conv2d_301[0][0]                 \n",
            "                                                                 activation_222[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_268 (BatchN (None, 32, 32, 64)   256         Block-0[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_42 (MaxPooling2D) (None, 16, 16, 64)   0           batch_normalization_268[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_303 (Conv2D)             (None, 16, 16, 128)  73728       max_pooling2d_42[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_224 (Activation)     (None, 16, 16, 128)  0           conv2d_303[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_269 (BatchN (None, 16, 16, 128)  512         activation_224[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_302 (Conv2D)             (None, 16, 16, 128)  8192        Block-0[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_304 (Conv2D)             (None, 16, 16, 128)  147456      batch_normalization_269[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_267 (BatchN (None, 16, 16, 128)  512         conv2d_302[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_61 (Add)                    (None, 16, 16, 128)  0           conv2d_304[0][0]                 \n",
            "                                                                 batch_normalization_267[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_225 (Activation)     (None, 16, 16, 128)  0           add_61[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_270 (BatchN (None, 16, 16, 128)  512         activation_225[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_305 (Conv2D)             (None, 16, 16, 128)  147456      batch_normalization_270[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_226 (Activation)     (None, 16, 16, 128)  0           conv2d_305[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_271 (BatchN (None, 16, 16, 128)  512         activation_226[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_306 (Conv2D)             (None, 16, 16, 128)  147456      batch_normalization_271[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Block-1 (Add)                   (None, 16, 16, 128)  0           conv2d_306[0][0]                 \n",
            "                                                                 activation_225[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_273 (BatchN (None, 16, 16, 128)  512         Block-1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_43 (MaxPooling2D) (None, 8, 8, 128)    0           batch_normalization_273[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_308 (Conv2D)             (None, 8, 8, 256)    294912      max_pooling2d_43[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_227 (Activation)     (None, 8, 8, 256)    0           conv2d_308[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_274 (BatchN (None, 8, 8, 256)    1024        activation_227[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_307 (Conv2D)             (None, 8, 8, 256)    32768       Block-1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_309 (Conv2D)             (None, 8, 8, 256)    589824      batch_normalization_274[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_272 (BatchN (None, 8, 8, 256)    1024        conv2d_307[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_62 (Add)                    (None, 8, 8, 256)    0           conv2d_309[0][0]                 \n",
            "                                                                 batch_normalization_272[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_228 (Activation)     (None, 8, 8, 256)    0           add_62[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_275 (BatchN (None, 8, 8, 256)    1024        activation_228[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_310 (Conv2D)             (None, 8, 8, 256)    589824      batch_normalization_275[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_229 (Activation)     (None, 8, 8, 256)    0           conv2d_310[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_276 (BatchN (None, 8, 8, 256)    1024        activation_229[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_311 (Conv2D)             (None, 8, 8, 256)    589824      batch_normalization_276[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Block-2 (Add)                   (None, 8, 8, 256)    0           conv2d_311[0][0]                 \n",
            "                                                                 activation_228[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_278 (BatchN (None, 8, 8, 256)    1024        Block-2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_44 (MaxPooling2D) (None, 4, 4, 256)    0           batch_normalization_278[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_313 (Conv2D)             (None, 4, 4, 512)    1179648     max_pooling2d_44[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_230 (Activation)     (None, 4, 4, 512)    0           conv2d_313[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_279 (BatchN (None, 4, 4, 512)    2048        activation_230[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_312 (Conv2D)             (None, 4, 4, 512)    131072      Block-2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_314 (Conv2D)             (None, 4, 4, 512)    2359296     batch_normalization_279[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_277 (BatchN (None, 4, 4, 512)    2048        conv2d_312[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_63 (Add)                    (None, 4, 4, 512)    0           conv2d_314[0][0]                 \n",
            "                                                                 batch_normalization_277[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_231 (Activation)     (None, 4, 4, 512)    0           add_63[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_280 (BatchN (None, 4, 4, 512)    2048        activation_231[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_315 (Conv2D)             (None, 4, 4, 512)    2359296     batch_normalization_280[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_232 (Activation)     (None, 4, 4, 512)    0           conv2d_315[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_281 (BatchN (None, 4, 4, 512)    2048        activation_232[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_316 (Conv2D)             (None, 4, 4, 512)    2359296     batch_normalization_281[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Block-3 (Add)                   (None, 4, 4, 512)    0           conv2d_316[0][0]                 \n",
            "                                                                 activation_231[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_13 (Gl (None, 512)          0           Block-3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 10)           5120        global_average_pooling2d_13[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "activation_233 (Activation)     (None, 10)           0           dense_11[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 11,181,760\n",
            "Trainable params: 11,173,056\n",
            "Non-trainable params: 8,704\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsoDVgOdQhle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CyclicLR(Callback):\n",
        "   \n",
        "\n",
        "    def __init__(self, base_lr=0.001, max_lr=0.05, step_size=2000., mode='exp_range',\n",
        "                 scale_fn=None, scale_mode='cycle'):\n",
        "\t\t\t\t \n",
        "        super(CyclicLR, self).__init__()\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        \n",
        "        if scale_fn == None:\n",
        "            \n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            \n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \"\"\"Resets cycle iterations.\n",
        "        Optional boundary/step size adjustment.\n",
        "        \"\"\"\n",
        "        if new_base_lr != None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size != None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "        \n",
        "    def clr(self):\n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "        return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "            print(' The learning rate used was ', self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())   \n",
        "            print(' The learning rate used was ', self.clr())\n",
        "            \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        "        #print(' The learning rate used was ', self.clr())\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(\" - lr: %0.5f \" % (self.history['lr'][-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0UMRrx-SQWG",
        "colab_type": "code",
        "outputId": "f28f0216-d1ec-4250-80a6-0fe10735bab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gc.collect()\n",
        "\n",
        "datagen_validation = ImageDataGenerator(featurewise_center=False,featurewise_std_normalization=False,)\n",
        "datagen_validation.fit(test_features)\n",
        "checkpoint = ModelCheckpoint(\"model_clr.hdf5\", monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "train_iterator = datagen.flow(train_features, train_labels, batch_size = batch_size,shuffle=True)\n",
        "validation_iterator = datagen_validation.flow(test_features, test_labels, batch_size=batch_size,shuffle=True)\n",
        "stopper = StopAtAccValue(validation_iterator, threshold = 0.9)\n",
        "\n",
        "cl = CyclicLR()\n",
        "callbacks=[cl,checkpoint,stopper]\n",
        "start_time = time.time()\n",
        "train_history = model.fit_generator(train_iterator,\n",
        "                    steps_per_epoch=len(train_iterator), \n",
        "                    validation_data = validation_iterator, \n",
        "                    validation_steps = len(validation_iterator),\n",
        "                    epochs=epochs, verbose=1,callbacks=callbacks)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"Time Taken = %.2f\"%(end_time-start_time))\n",
        "\n",
        "# model.load_weights(\"model.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " The learning rate used was  0.001\n",
            "Epoch 1/100\n",
            "  1/391 [..............................] - ETA: 38:16 - loss: 3.7330 - acc: 0.1484WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.233146). Check your callbacks.\n",
            "390/391 [============================>.] - ETA: 0s - loss: 2.7183 - acc: 0.3383Epoch 1/100\n",
            " 78/391 [====>.........................] - ETA: 12s - loss: 2.9347 - acc: 0.3232 - lr: 0.01056 \n",
            "Continue Training:: Val Acc = 0.323, Threshold = 0.900\n",
            "391/391 [==============================] - 57s 146ms/step - loss: 2.7177 - acc: 0.3387 - val_loss: 2.9376 - val_acc: 0.3230\n",
            "Epoch 2/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 2.2622 - acc: 0.5081Epoch 1/100\n",
            " 77/391 [====>.........................] - ETA: 6s - loss: 2.2751 - acc: 0.5187 - lr: 0.02013 \n",
            "Continue Training:: Val Acc = 0.519, Threshold = 0.900\n",
            "391/391 [==============================] - 31s 78ms/step - loss: 2.2619 - acc: 0.5081 - val_loss: 2.2870 - val_acc: 0.5192\n",
            "Epoch 3/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.9683 - acc: 0.6057Epoch 1/100\n",
            " 77/391 [====>.........................] - ETA: 6s - loss: 1.8984 - acc: 0.6427 - lr: 0.02971 \n",
            "Continue Training:: Val Acc = 0.643, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 78ms/step - loss: 1.9678 - acc: 0.6060 - val_loss: 1.8967 - val_acc: 0.6426\n",
            "Epoch 4/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.7544 - acc: 0.6706Epoch 1/100\n",
            " 78/391 [====>.........................] - ETA: 6s - loss: 1.6787 - acc: 0.7020 - lr: 0.03929 \n",
            "Continue Training:: Val Acc = 0.702, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 78ms/step - loss: 1.7544 - acc: 0.6706 - val_loss: 1.6782 - val_acc: 0.7020\n",
            "Epoch 5/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.6037 - acc: 0.7122Epoch 1/100\n",
            " 78/391 [====>.........................] - ETA: 7s - loss: 1.7403 - acc: 0.6876 - lr: 0.04887 \n",
            "Continue Training:: Val Acc = 0.688, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 1.6041 - acc: 0.7121 - val_loss: 1.7396 - val_acc: 0.6878\n",
            "Epoch 6/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.4823 - acc: 0.7408Epoch 1/100\n",
            " 77/391 [====>.........................] - ETA: 7s - loss: 1.4370 - acc: 0.7561 - lr: 0.04155 \n",
            "Continue Training:: Val Acc = 0.756, Threshold = 0.900\n",
            "391/391 [==============================] - 31s 78ms/step - loss: 1.4820 - acc: 0.7409 - val_loss: 1.4441 - val_acc: 0.7555\n",
            "Epoch 7/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.3430 - acc: 0.7776Epoch 1/100\n",
            " 79/391 [=====>........................] - ETA: 6s - loss: 1.2480 - acc: 0.8124 - lr: 0.03197 \n",
            "Continue Training:: Val Acc = 0.812, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 1.3429 - acc: 0.7775 - val_loss: 1.2480 - val_acc: 0.8124\n",
            "Epoch 8/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.2345 - acc: 0.8056Epoch 1/100\n",
            " 78/391 [====>.........................] - ETA: 6s - loss: 1.1780 - acc: 0.8259 - lr: 0.02239 \n",
            "Continue Training:: Val Acc = 0.826, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 1.2343 - acc: 0.8056 - val_loss: 1.1772 - val_acc: 0.8260\n",
            "Epoch 9/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.1493 - acc: 0.8300Epoch 1/100\n",
            " 78/391 [====>.........................] - ETA: 6s - loss: 1.0963 - acc: 0.8514 - lr: 0.01281 \n",
            "Continue Training:: Val Acc = 0.851, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 78ms/step - loss: 1.1493 - acc: 0.8299 - val_loss: 1.0984 - val_acc: 0.8513\n",
            "Epoch 10/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.0788 - acc: 0.8525Epoch 1/100\n",
            " 78/391 [====>.........................] - ETA: 6s - loss: 1.0521 - acc: 0.8642 - lr: 0.00323 \n",
            "Continue Training:: Val Acc = 0.864, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 1.0785 - acc: 0.8527 - val_loss: 1.0548 - val_acc: 0.8641\n",
            "Epoch 11/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.0367 - acc: 0.8642Epoch 1/100\n",
            " 79/391 [=====>........................] - ETA: 7s - loss: 1.0606 - acc: 0.8638 - lr: 0.00835 \n",
            "Continue Training:: Val Acc = 0.864, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 1.0366 - acc: 0.8642 - val_loss: 1.0606 - val_acc: 0.8638\n",
            "Epoch 12/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.0488 - acc: 0.8569Epoch 1/100\n",
            " 79/391 [=====>........................] - ETA: 6s - loss: 1.0629 - acc: 0.8610 - lr: 0.01793 \n",
            "Continue Training:: Val Acc = 0.861, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 1.0489 - acc: 0.8568 - val_loss: 1.0629 - val_acc: 0.8610\n",
            "Epoch 13/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.0609 - acc: 0.8486Epoch 1/100\n",
            " 77/391 [====>.........................] - ETA: 7s - loss: 1.1256 - acc: 0.8290 - lr: 0.02751 \n",
            "Continue Training:: Val Acc = 0.829, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 1.0610 - acc: 0.8486 - val_loss: 1.1310 - val_acc: 0.8288\n",
            "Epoch 14/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.0655 - acc: 0.8444Epoch 1/100\n",
            " 77/391 [====>.........................] - ETA: 7s - loss: 1.0936 - acc: 0.8410 - lr: 0.03709 \n",
            "Continue Training:: Val Acc = 0.841, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 1.0655 - acc: 0.8444 - val_loss: 1.1003 - val_acc: 0.8411\n",
            "Epoch 15/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.0680 - acc: 0.8372Epoch 1/100\n",
            " 79/391 [=====>........................] - ETA: 7s - loss: 1.1259 - acc: 0.8315 - lr: 0.04667 \n",
            "Continue Training:: Val Acc = 0.831, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 1.0680 - acc: 0.8373 - val_loss: 1.1259 - val_acc: 0.8315\n",
            "Epoch 16/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.0541 - acc: 0.8360Epoch 1/100\n",
            " 79/391 [=====>........................] - ETA: 6s - loss: 1.0439 - acc: 0.8456 - lr: 0.04375 \n",
            "Continue Training:: Val Acc = 0.846, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 1.0539 - acc: 0.8361 - val_loss: 1.0439 - val_acc: 0.8456\n",
            "Epoch 17/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.9948 - acc: 0.8524Epoch 1/100\n",
            " 78/391 [====>.........................] - ETA: 6s - loss: 1.0448 - acc: 0.8431 - lr: 0.03417 \n",
            "Continue Training:: Val Acc = 0.843, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 0.9949 - acc: 0.8524 - val_loss: 1.0490 - val_acc: 0.8432\n",
            "Epoch 18/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.9310 - acc: 0.8719Epoch 1/100\n",
            " 78/391 [====>.........................] - ETA: 6s - loss: 0.9912 - acc: 0.8602 - lr: 0.02459 \n",
            "Continue Training:: Val Acc = 0.860, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 0.9311 - acc: 0.8718 - val_loss: 0.9945 - val_acc: 0.8601\n",
            "Epoch 19/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.8698 - acc: 0.8901Epoch 1/100\n",
            " 78/391 [====>.........................] - ETA: 6s - loss: 0.9259 - acc: 0.8767 - lr: 0.01501 \n",
            "Continue Training:: Val Acc = 0.877, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 0.8697 - acc: 0.8900 - val_loss: 0.9257 - val_acc: 0.8767\n",
            "Epoch 20/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.8255 - acc: 0.9040Epoch 1/100\n",
            " 79/391 [=====>........................] - ETA: 6s - loss: 0.8906 - acc: 0.8897 - lr: 0.00543 \n",
            "Continue Training:: Val Acc = 0.890, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 0.8253 - acc: 0.9041 - val_loss: 0.8906 - val_acc: 0.8897\n",
            "Epoch 21/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.7971 - acc: 0.9132Epoch 1/100\n",
            " 79/391 [=====>........................] - ETA: 7s - loss: 0.8901 - acc: 0.8911 - lr: 0.00614 \n",
            "Continue Training:: Val Acc = 0.891, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 0.7970 - acc: 0.9132 - val_loss: 0.8901 - val_acc: 0.8911\n",
            "Epoch 22/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.8070 - acc: 0.9084Epoch 1/100\n",
            " 79/391 [=====>........................] - ETA: 6s - loss: 0.9071 - acc: 0.8823 - lr: 0.01572 \n",
            "Continue Training:: Val Acc = 0.882, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 0.8069 - acc: 0.9085 - val_loss: 0.9071 - val_acc: 0.8823\n",
            "Epoch 23/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.8146 - acc: 0.9035Epoch 1/100\n",
            " 78/391 [====>.........................] - ETA: 6s - loss: 0.9692 - acc: 0.8645 - lr: 0.02530 \n",
            "Continue Training:: Val Acc = 0.864, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 0.8143 - acc: 0.9035 - val_loss: 0.9745 - val_acc: 0.8645\n",
            "Epoch 24/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.8331 - acc: 0.8932Epoch 1/100\n",
            " 78/391 [====>.........................] - ETA: 6s - loss: 0.9996 - acc: 0.8524 - lr: 0.03488 \n",
            "Continue Training:: Val Acc = 0.852, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 76ms/step - loss: 0.8333 - acc: 0.8932 - val_loss: 1.0023 - val_acc: 0.8523\n",
            "Epoch 25/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.8520 - acc: 0.8836Epoch 1/100\n",
            " 78/391 [====>.........................] - ETA: 6s - loss: 0.9978 - acc: 0.8588 - lr: 0.04446 \n",
            "Continue Training:: Val Acc = 0.859, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 0.8522 - acc: 0.8835 - val_loss: 1.0004 - val_acc: 0.8587\n",
            "Epoch 26/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.8618 - acc: 0.8782Epoch 1/100\n",
            " 77/391 [====>.........................] - ETA: 7s - loss: 0.9876 - acc: 0.8495 - lr: 0.04596 \n",
            "Continue Training:: Val Acc = 0.850, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 0.8620 - acc: 0.8781 - val_loss: 0.9891 - val_acc: 0.8498\n",
            "Epoch 27/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.8239 - acc: 0.8895Epoch 1/100\n",
            " 77/391 [====>.........................] - ETA: 7s - loss: 0.8961 - acc: 0.8762 - lr: 0.03638 \n",
            "Continue Training:: Val Acc = 0.876, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 0.8240 - acc: 0.8895 - val_loss: 0.9005 - val_acc: 0.8760\n",
            "Epoch 28/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.7710 - acc: 0.9044Epoch 1/100\n",
            " 78/391 [====>.........................] - ETA: 6s - loss: 0.8811 - acc: 0.8826 - lr: 0.02680 \n",
            "Continue Training:: Val Acc = 0.882, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 76ms/step - loss: 0.7707 - acc: 0.9045 - val_loss: 0.8827 - val_acc: 0.8825\n",
            "Epoch 29/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.7374 - acc: 0.9149Epoch 1/100\n",
            " 76/391 [====>.........................] - ETA: 7s - loss: 0.8607 - acc: 0.8817 - lr: 0.01722 \n",
            "Continue Training:: Val Acc = 0.881, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 0.7376 - acc: 0.9148 - val_loss: 0.8665 - val_acc: 0.8812\n",
            "Epoch 30/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.7002 - acc: 0.9273Epoch 1/100\n",
            " 77/391 [====>.........................] - ETA: 6s - loss: 0.8343 - acc: 0.8938 - lr: 0.00764 \n",
            "Continue Training:: Val Acc = 0.894, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 0.6999 - acc: 0.9274 - val_loss: 0.8350 - val_acc: 0.8939\n",
            "Epoch 31/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.6737 - acc: 0.9358Epoch 1/100\n",
            " 77/391 [====>.........................] - ETA: 7s - loss: 0.8259 - acc: 0.8961 - lr: 0.00394 \n",
            "Continue Training:: Val Acc = 0.896, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 0.6738 - acc: 0.9358 - val_loss: 0.8270 - val_acc: 0.8962\n",
            "Epoch 32/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.6726 - acc: 0.9364Epoch 1/100\n",
            " 78/391 [====>.........................] - ETA: 6s - loss: 0.8374 - acc: 0.8921 - lr: 0.01352 \n",
            "Continue Training:: Val Acc = 0.892, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 76ms/step - loss: 0.6724 - acc: 0.9364 - val_loss: 0.8388 - val_acc: 0.8921\n",
            "Epoch 33/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.6821 - acc: 0.9314Epoch 1/100\n",
            " 78/391 [====>.........................] - ETA: 6s - loss: 0.8759 - acc: 0.8883 - lr: 0.02310 \n",
            "Continue Training:: Val Acc = 0.888, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 0.6821 - acc: 0.9314 - val_loss: 0.8718 - val_acc: 0.8885\n",
            "Epoch 34/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.6957 - acc: 0.9245Epoch 1/100\n",
            " 78/391 [====>.........................] - ETA: 6s - loss: 0.8678 - acc: 0.8869 - lr: 0.03268 \n",
            "Continue Training:: Val Acc = 0.887, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 76ms/step - loss: 0.6958 - acc: 0.9245 - val_loss: 0.8651 - val_acc: 0.8869\n",
            "Epoch 35/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.7154 - acc: 0.9145Epoch 1/100\n",
            " 77/391 [====>.........................] - ETA: 7s - loss: 0.8996 - acc: 0.8774 - lr: 0.04226 \n",
            "Continue Training:: Val Acc = 0.877, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 0.7154 - acc: 0.9146 - val_loss: 0.8964 - val_acc: 0.8774\n",
            "Epoch 36/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.7298 - acc: 0.9079Epoch 1/100\n",
            " 77/391 [====>.........................] - ETA: 7s - loss: 0.9028 - acc: 0.8750 - lr: 0.04816 \n",
            "Continue Training:: Val Acc = 0.875, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 0.7297 - acc: 0.9079 - val_loss: 0.9005 - val_acc: 0.8752\n",
            "Epoch 37/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.7156 - acc: 0.9120Epoch 1/100\n",
            " 79/391 [=====>........................] - ETA: 6s - loss: 0.8678 - acc: 0.8798 - lr: 0.03858 \n",
            "Continue Training:: Val Acc = 0.880, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 0.7154 - acc: 0.9120 - val_loss: 0.8678 - val_acc: 0.8798\n",
            "Epoch 38/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.6794 - acc: 0.9237Epoch 1/100\n",
            " 79/391 [=====>........................] - ETA: 6s - loss: 0.8407 - acc: 0.8842 - lr: 0.02900 \n",
            "Continue Training:: Val Acc = 0.884, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 0.6795 - acc: 0.9236 - val_loss: 0.8407 - val_acc: 0.8842\n",
            "Epoch 39/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.6440 - acc: 0.9346Epoch 1/100\n",
            " 78/391 [====>.........................] - ETA: 6s - loss: 0.8144 - acc: 0.8965 - lr: 0.01942 \n",
            "Continue Training:: Val Acc = 0.897, Threshold = 0.900\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 0.6441 - acc: 0.9345 - val_loss: 0.8114 - val_acc: 0.8966\n",
            "Epoch 40/100\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.6167 - acc: 0.9429Epoch 1/100\n",
            " 78/391 [====>.........................] - ETA: 6s - loss: 0.7944 - acc: 0.9013 - lr: 0.00984 \n",
            "Stopping Training:: Val Acc = 0.901 Achieved\n",
            "391/391 [==============================] - 30s 77ms/step - loss: 0.6167 - acc: 0.9428 - val_loss: 0.7908 - val_acc: 0.9015\n",
            "Time Taken = 1236.05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN43Jew1bdAc",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJxv5CsKMUSx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}